<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>CUDA Course V2</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(0, 0, 0, 0.06); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="a67249b9-e99c-49d2-83af-61f02d52f186" class="page sans"><header><h1 class="page-title">CUDA Course V2</h1><p class="page-description"></p></header><div class="page-body"><details open=""><summary style="font-weight:600;font-size:1.875em;line-height:1.3;margin:0">Todos</summary><div class="indented"><ul id="080392a3-7164-478f-a25e-b3f1dab8ad54" class="bulleted-list"><li style="list-style-type:disc">revamp DL ecosystem notes</li></ul><ul id="602d281c-0264-4d92-ab51-ed02a101bc37" class="bulleted-list"><li style="list-style-type:disc">struct ptrs ⇒ <code>nn.w1</code> vs <code>nn-&gt;w1</code></li></ul><ul id="e195d989-a7b0-4fae-a9fe-c66f8d181019" class="bulleted-list"><li style="list-style-type:disc">NCCL &amp; cuBLAS-Xt fundamental operations<ul id="a0db0318-59b1-438e-801e-630ef95bc952" class="bulleted-list"><li style="list-style-type:circle">cublas-xt vs cublasMP</li></ul></li></ul><ul id="7dc7c01b-ea5b-4c38-a9f2-c92bfc03570c" class="bulleted-list"><li style="list-style-type:disc">pytorch extensions C++</li></ul><ul id="584b649f-45fb-400d-8691-ff5fc2def944" class="bulleted-list"><li style="list-style-type:disc">nvtx profiler</li></ul><ul id="7de082da-24cd-4caf-921e-69aa4fb612a3" class="bulleted-list"><li style="list-style-type:disc">mlir vs llvm</li></ul><ul id="4fed55c6-d071-4b9b-881c-e11adb8bd8df" class="bulleted-list"><li style="list-style-type:disc"></li></ul><ul id="3202854c-19ef-44dc-b286-5e1678f112dd" class="bulleted-list"><li style="list-style-type:disc">transfer all this to github with the following structure<ul id="d0f7dace-1023-40c6-92eb-3fb1d9c39914" class="bulleted-list"><li style="list-style-type:circle"><a href="http://README.md">README.md</a> (intro stuff)</li></ul><ul id="9e474508-aebe-4a40-92f4-1953ac618336" class="bulleted-list"><li style="list-style-type:circle">chapters in folders<ul id="0d2e85cf-ca5f-4d34-9dfc-1e49cbb75376" class="bulleted-list"><li style="list-style-type:square">assets for that chapter (imgs)</li></ul><ul id="b8ad310e-71cd-4ce2-af77-c76779517e08" class="bulleted-list"><li style="list-style-type:square">code files w/ docs</li></ul><ul id="3105a87d-e4c5-420e-b5d7-45e376617373" class="bulleted-list"><li style="list-style-type:square">README.md</li></ul></li></ul></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.5em;line-height:1.3;margin:0">Explainer words</summary><div class="indented"><ul id="7d9c909d-40da-4b61-8645-88514209aaca" class="bulleted-list"><li style="list-style-type:disc">analogously</li></ul><ul id="86181c02-484a-4b9e-9aa2-88dbeedae6f4" class="bulleted-list"><li style="list-style-type:disc">map x onto y</li></ul><ul id="b9706d94-4c89-419d-9b93-b1fe898609f3" class="bulleted-list"><li style="list-style-type:disc">mapping</li></ul><ul id="3e099c13-2ce4-42a6-a1dc-aa242b1a0ae2" class="bulleted-list"><li style="list-style-type:disc">to iterate</li></ul><ul id="f52971b3-955f-4b47-a22d-d67d19534cf1" class="bulleted-list"><li style="list-style-type:disc">use geometry ⇒ arrow, cube, line, arrow</li></ul><ul id="4cd5399c-4046-451d-a915-541e6c7cc39d" class="bulleted-list"><li style="list-style-type:disc">feeds into</li></ul><ul id="62ec3316-e3aa-4e90-a7e5-383c510887cf" class="bulleted-list"><li style="list-style-type:disc">plug in</li></ul><ul id="3e0adca9-c8c3-4719-88e2-83da2be4f360" class="bulleted-list"><li style="list-style-type:disc">insert</li></ul><ul id="3cd564a6-6d07-439c-8b8e-669e0517d9ac" class="bulleted-list"><li style="list-style-type:disc">jump</li></ul><ul id="a3869245-b5f5-4e22-b349-260d6d90bc1e" class="bulleted-list"><li style="list-style-type:disc">attach/detach</li></ul><ul id="a5505c3c-2a29-4a97-9cb2-e397da42bf71" class="bulleted-list"><li style="list-style-type:disc">in the context of</li></ul><ul id="21dcb8ea-2daf-4a58-a706-af85eca8d5cd" class="bulleted-list"><li style="list-style-type:disc">over/underfitting</li></ul><ul id="311c5251-4f6f-4b11-95bf-33bfcc70ca64" class="bulleted-list"><li style="list-style-type:disc">black box</li></ul><ul id="0d70ca9f-c160-4248-b069-6dbce243f0a9" class="bulleted-list"><li style="list-style-type:disc">derivative is just x, gradient is x, y, z, etc. I will use them interchangeably</li></ul><ul id="f9cd518b-6ad4-4466-8d37-d89e75907bc2" class="bulleted-list"><li style="list-style-type:disc">SGEMM ⇒ Single-precison GEneral MatMul ( <em>C </em>= <em>α </em>× <em>A </em>× <em>B </em>+ <em>β </em>× <em>C )</em></li></ul><ul id="5d750079-d0a9-40db-b807-495acb5c6ae3" class="bulleted-list"><li style="list-style-type:disc">refer to lecture X instead of earlier in the course to be more explicit</li></ul><ul id="ef798b3f-f4cd-43a4-b738-076d148c2513" class="bulleted-list"><li style="list-style-type:disc"><code>nvitop</code> for cool graphs in terminal (opt.)</li></ul><ul id="348aaff6-7b6c-4355-9f57-edee9de3196f" class="bulleted-list"><li style="list-style-type:disc">planning to transfer this to github later when the free course comes out</li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.5em;line-height:1.3;margin:0">Intro</summary><div class="indented"><ul id="106f8904-0fe8-4990-aa70-6a79a312fabd" class="bulleted-list"><li style="list-style-type:disc">introduce myself (craft this later)<ul id="b08bd3db-2b57-4ba6-bcb6-cd384be20b3e" class="bulleted-list"><li style="list-style-type:circle">possibly leave out qualifications and keep simple “Hi im Elliot and today we are going to learn about CUDA”</li></ul><ul id="f5f230bb-ea08-4392-a7f0-caae639be64a" class="bulleted-list"><li style="list-style-type:circle">look at other intros (wesboss)</li></ul><ul id="1158015d-5cd5-490b-85d3-d33ddf3f9a4a" class="bulleted-list"><li style="list-style-type:circle">not always going to be who I am when it is published at the time it is.</li></ul></li></ul><ul id="e57ceba0-7d26-4c74-9e87-da203384aa2c" class="bulleted-list"><li style="list-style-type:disc">showcase MNIST computer vision training + inference at the end</li></ul><ul id="09059673-2536-4a55-9035-8bfb52ffdfa9" class="toggle"><li><details open=""><summary>In an undecided order</summary><ul id="6fe8bf06-226c-455e-9f4c-4c08afb60b8c" class="bulleted-list"><li style="list-style-type:disc">Why did I create this course? ⇒ barrier to entry is very high for kernel engineer jobs OPENAI (<a href="https://openai.com/careers/gpu-kernels-engineer/">https://openai.com/careers/gpu-kernels-engineer/</a>) and NVIDIA High Performance Computing Engineering roles (<a href="https://nvidia.wd5.myworkdayjobs.com/en-US/NVIDIAExternalCareerSite/details/Senior-HPC-Performance-Engineer_JR1977468">https://nvidia.wd5.myworkdayjobs.com/en-US/NVIDIAExternalCareerSite/details/Senior-HPC-Performance-Engineer_JR1977468</a>). </li></ul><ul id="3ee14beb-e444-4fa8-9bca-ca1f99f3053e" class="bulleted-list"><li style="list-style-type:disc">generally speaking the point of writing GPU kernels or playing with code AT ALL on the gpu is to run something faster. take a nested for loop in python to do some linear algebra. your job is to use your knowledge GPU architecture, kernel launch configuations, and a bunch of other cool stuff we cover in this course to make that code run as fast as possible.</li></ul><ul id="e2195d51-e2ef-44fd-8042-757e20abe38e" class="bulleted-list"><li style="list-style-type:disc">I want to note early on that this course won’t be CUDA only. we reference pytorch a lot and even a bit of triton too. I will cover the the terms and ecosystem so it makes more sense as you progress through the material. </li></ul><ul id="4ff31a5d-ce09-46a9-937c-46ae2d5f4c9a" class="bulleted-list"><li style="list-style-type:disc">We will not be writing extensive training tests to deploy a model, but rather focusing on the technical details of writing faster kernels. The deployment part will be one of the first parts covered in the course. It would be weird to show you how to write really fast kernels and not plant a seed on how you could use those the tools in the deep learning ecosystem to significantly cut costs. </li></ul><ul id="09323050-3c36-447c-8fbe-ee31749be1a4" class="bulleted-list"><li style="list-style-type:disc">This is strictly for nvidia gpus, if you don’t have one, consider renting the cheapest ones in the cloud and playing around. I advise you look into the pricing before giving a definite “NO”. at first, I was surprised at how low the cost for some cloud services where, especially the non-compute demanding ones. you are just experimenting, not writing the training run for GPT-5</li></ul><ul id="f7d88f0c-46b8-4c7e-88f4-fe25b7d5e05a" class="bulleted-list"><li style="list-style-type:disc">We will soon go over the table of contents, in which we end off with a simple CNN project in CUDA. Instead of making a mini project on the common decoder-only transformer architecture, I want to cover something else so karpathy’s llm.c (<a href="https://github.com/karpathy/llm.c">https://github.com/karpathy/llm.c</a>) will be a fluid transition after coming out of this. I’m not certain when or if lectures on llm.c will be released at all but to make this course worth your time, I cover many things, giving you the fundamentals to go and read llm.c on your own.</li></ul><ul id="584d3888-380d-4de7-bd0b-074cbfbd51ac" class="bulleted-list"><li style="list-style-type:disc">As for course prerequisites, python programming will help in understanding what we are implementing in lower-level languages. Basic differentiation and vector calculus will make learning easier, but it really only required for intuition behind backpropagation. Linear algebra will make your life easier by not having to learn the fundamental algorithms from scratch, but I will go over them anyways. If you really care, review the following: matrix transpose, matrix multiplication, chain rule from calculus, gradient vs derivative… MAYBE MORE</li></ul><ul id="fed1e094-9ad5-499f-9974-f5f29c9cca3e" class="bulleted-list"><li style="list-style-type:disc">Nowadays, they say we have too much data, but very little cleaned data. I have taken everything from all the other videos/courses on youtube and elsewhere and put the must haves into a single course. This includes the topics covered by paid courses too. I have the links for youtube videos I’ve parsed through right here so you don’t have to spend hours doing the same.</li></ul><ul id="7e25d0a5-9ba9-4431-86f7-f595b72c5091" class="bulleted-list"><li style="list-style-type:disc">I suggest following this notion doc to maintain a structured learning approach. I may also include intuitive excalidraw drawings and diagrams to break down the complex of material in this course ⇒ which I will also provide links to (join my discord server to find the links). When you can visualize the geometrical interaction of the topics presented in this course, you will likely find parallel programming extremely fun.</li></ul><ul id="6834eedf-7cf3-414c-91bf-aafeafa4b0d9" class="bulleted-list"><li style="list-style-type:disc">Some key takeaways from this course would be to take an existing implementation and make it faster, or to look at current research not yet done, and build a CUDA kernel to make it run as fast as possible. I will cover both the profiling and optimization aspect, as well as implementing neural network functions like backpropagation, batchnorm, convolutions, activations, and more.</li></ul><ul id="116057f9-e080-4d16-ad0d-58236b0aba51" class="bulleted-list"><li style="list-style-type:disc">SPOILER ALERT… through experimentation and research, you will learn that the main GPU performance bottleneck is memory bandwidth. in deep learning, we have these giant inscrutable matrices that cannot fit into the on chip memory at once. we have to take little chunks of them at a time from off-chip memory. You may think the VRAM is fast, but its actually the main bottleneck of many deep learning GPU applications. it takes a really long time to copy/transfer data over to the cores from this off-chip VRAM. by off-chip, I mean the memory cells aren’t close to the cores, wheras SRAM and register memory is directly on chip (very close to cores). to illusrate, the SRAM has a memory bandwidth of about 20 terabytes a second, whereas VRAM is about 100 gigabytes per second. we end up with a bunch of super fast CUDA cores waiting for data to arrive rather than doing constantly computation.  </li></ul><ul id="eae6d78b-da7e-4287-8062-e3061614766d" class="bulleted-list"><li style="list-style-type:disc">you can continue run with any NVIDIA GTX, RTX, or datacenter level GPU.</li></ul><ul id="8f035bfd-b10f-485f-a991-3efd51143092" class="bulleted-list"><li style="list-style-type:disc">Use cases for CUDA / Parallel / GPU Programming<ul id="51f9e59e-c17d-40b9-ac7d-d1e359fa80c3" class="bulleted-list"><li style="list-style-type:circle">Other use cases (opt. might omit later)<ul id="667db7ba-05a9-48bb-8c2f-b318e6428d18" class="bulleted-list"><li style="list-style-type:square">Graphics and Ray-tracing<ul id="a186f9b5-19d6-4461-b022-bb5d6d0b16d1" class="bulleted-list"><li style="list-style-type:disc">Simply, <strong>OpenGL draws everything on your screen really fast, OpenCL and CUDA process the calculations necessary when your videos interact with your effects and other media</strong>.</li></ul></li></ul><ul id="be73eff1-4079-4f77-923d-6a9a5a0ec909" class="bulleted-list"><li style="list-style-type:square">Fluid Simulation</li></ul><ul id="37ba949d-e6fe-4873-9021-a3a1838aaa06" class="bulleted-list"><li style="list-style-type:square">Video Editting</li></ul><ul id="f813630c-94a7-461d-8e2e-80fd4168d072" class="bulleted-list"><li style="list-style-type:square">Crypto Mining</li></ul><ul id="fc65ebc9-3f08-4ad0-9f9f-01728a0bece5" class="bulleted-list"><li style="list-style-type:square">3D modelling in Software like Blender</li></ul></li></ul><ul id="e21d04ec-dbf2-4719-b653-ddb055286047" class="bulleted-list"><li style="list-style-type:circle">This course<ul id="28477bd9-3b04-4a30-bc89-c6b151149aa0" class="bulleted-list"><li style="list-style-type:square">You guessed it! …Deep Learning the #1 use case for CUDA is primarily what I’ll be covering in this course</li></ul></li></ul><ul id="7b240cd4-591e-4cd8-9102-0500df0b5bdf" class="bulleted-list"><li style="list-style-type:circle">Also building up GPU clusters for multi-node parallelism but their are docs for this</li></ul></li></ul><ul id="6110be79-316b-4ba7-a293-c666226d168a" class="bulleted-list"><li style="list-style-type:disc">check github, stackoverflow, nvidia developer forums, nvidia docs, pytorch docs if your issue is related to CUDA OR triton in pytorch, chatGPT or other LLMs to help navigate the space more easily (information won’t be as hard to process since its neatly organized). this is a part of being a programmer :)</li></ul><ul id="75d237d1-d982-480c-b6e3-8d8dc06a3ec8" class="bulleted-list"><li style="list-style-type:disc">all the code and notes for this course are kept in the github repo in the description. the ecosystem changes over time so if you’re looking at this a couple years from now it might be outdated. I’ll make sure to publish working code to github repo regardless of what happens to the ecosystem. this may make it slightly confusing at times. but you should be able to reproduce all the material with the contents in the github repo.</li></ul><ul id="f5d48713-bc18-4ee4-8972-223acb08506f" class="bulleted-list"><li style="list-style-type:disc">Heads up, I don’t cover anything about windows (even though it is possible). because I do everything on ubuntu linux. You can use Linux Subsystem for windows if you cannot switch to ubuntu ⇒ <a href="https://learn.microsoft.com/en-us/windows/wsl/install">https://learn.microsoft.com/en-us/windows/wsl/install</a> &amp; <a href="https://docs.nvidia.com/cuda/wsl-user-guide/index.html">https://docs.nvidia.com/cuda/wsl-user-guide/index.html</a>. you can also fire up a ubuntu docker container and get access to all linux features there (without UIs)</li></ul><ul id="5a0f19f2-5266-4218-b8c1-d5378fcb3de9" class="toggle"><li><details open=""><summary>Go over the course structure (Table of Contents)</summary><ul id="3b3d225b-9b4b-4750-aa77-2ea14061260e" class="bulleted-list"><li style="list-style-type:disc"><span style="border-bottom:0.05em solid">SHORT FORM:</span><ul id="c9bb6647-d64d-4e7e-8231-97dc212d567b" class="bulleted-list"><li style="list-style-type:circle">this (the intro)</li></ul><ul id="d513d584-f063-4fc8-b244-06bc161eb555" class="bulleted-list"><li style="list-style-type:circle">briefly touching on C/C++</li></ul><ul id="8f10c010-878d-4056-a7f9-e9485a63334d" class="bulleted-list"><li style="list-style-type:circle">soft intro to GPUs</li></ul><ul id="6a8a6a79-b95e-44e1-bbf4-f33785c2b179" class="bulleted-list"><li style="list-style-type:circle">writing your first CUDA kernels with the fundamentals principles of the CUDA language</li></ul><ul id="51fcc41a-3024-4428-ae0a-c8b65e37532a" class="bulleted-list"><li style="list-style-type:circle">Stepping up to the CUDA libraries</li></ul><ul id="18c1a2b1-52d1-48eb-aec0-0ba2bec98e40" class="bulleted-list"><li style="list-style-type:circle">making the matrix multiply and FFT faster by starting with a 20 line implementation and adding little “hacks” step by step</li></ul><ul id="045f1ca5-8e06-432f-9045-a9405c03c6bb" class="bulleted-list"><li style="list-style-type:circle">customizing/adding to pytorch with the kernel knowledge you have up to this point</li></ul><ul id="89631b5c-efa9-4e6e-ba11-27cfa198b9cf" class="bulleted-list"><li style="list-style-type:circle">etc…</li></ul></li></ul></details></li></ul><ul id="8d3e7cdb-3d62-45ce-b6d3-32fef836b74e" class="bulleted-list"><li style="list-style-type:disc"><em><strong><span style="border-bottom:0.05em solid">more stuff I haven’t thought of</span></strong></em></li></ul><ul id="7314866f-f516-44d7-9ac0-2159d534c20b" class="toggle"><li><details open=""><summary>HERE (get a taste for how cool GPU programming is)</summary><figure id="dadeda32-1ac9-4112-9747-ae1335e341d0"><div class="source"><a href="https://www.youtube.com/watch?v=Ys4rFt8XbMM&amp;ab_channel=JacobRintamaki">https://www.youtube.com/watch?v=Ys4rFt8XbMM&amp;ab_channel=JacobRintamaki</a></div></figure><figure id="d635c442-92a9-47d4-9839-ef7541fba580"><div class="source"><a href="https://www.youtube.com/watch?v=QQceTDjA4f4&amp;ab_channel=ChristopherHollinworth">https://www.youtube.com/watch?v=QQceTDjA4f4&amp;ab_channel=ChristopherHollinworth</a></div></figure><figure id="f29b54e6-632c-4f8f-bf2f-0474bbfdd3b1"><div class="source"><a href="https://youtu.be/zSCdTOKrnII">https://youtu.be/zSCdTOKrnII</a></div></figure><figure id="92f00fe8-7c5e-45f1-a203-f079ceb68b25"><div class="source"><a href="https://youtu.be/r5NQecwZs1A">https://youtu.be/r5NQecwZs1A</a></div></figure><figure id="a644092a-fb05-4858-85c9-0117ef69f4c1"><div class="source"><a href="https://youtu.be/pPStdjuYzSI">https://youtu.be/pPStdjuYzSI</a></div></figure><figure id="401f3ef7-fc22-4989-a1ad-9160edde7aca"><div class="source"><a href="https://youtu.be/fDAPJ7rvcUw">https://youtu.be/fDAPJ7rvcUw</a></div></figure><figure id="ac5a8d46-fba5-41ab-8dd1-8479f35b7354"><div class="source"><a href="https://youtu.be/sZxjuT1kUd0">https://youtu.be/sZxjuT1kUd0</a></div></figure><figure id="611dffc2-ffe0-4c8d-a403-23721c5f6b34"><div class="source"><a href="https://youtu.be/ga2ML1uGr5o">https://youtu.be/ga2ML1uGr5o</a></div></figure><figure id="cf0a7653-67ff-4f6e-a8a6-9af8f5454e14"><div class="source"><a href="https://youtu.be/DpEgZe2bbU0">https://youtu.be/DpEgZe2bbU0</a></div></figure><figure id="76b6e6e4-8e92-4f6e-b005-e632e67d3bca"><div class="source"><a href="https://youtu.be/G-EimI4q-TQ">https://youtu.be/G-EimI4q-TQ</a></div></figure><figure id="59413e3d-014b-4e1a-bf8f-4df68ece2222"><div class="source"><a href="https://youtu.be/xwbD6fL5qC8">https://youtu.be/xwbD6fL5qC8</a></div></figure><figure id="1581aec0-5b85-49e5-a456-0c3dc3d28f09"><div class="source"><a href="https://youtu.be/4APkMJdiudU">https://youtu.be/4APkMJdiudU</a></div></figure><figure id="a13ca689-1a0a-4b50-9617-e7d2b94d3e2e"><div class="source"><a href="https://www.youtube.com/watch?v=kUqkOAU84bA">https://www.youtube.com/watch?v=kUqkOAU84bA</a></div></figure></details></li></ul><ul id="aff997f3-61a0-4df1-8493-25ff87ac5855" class="toggle"><li><details open=""><summary>Github/other links</summary><figure id="32e7f210-a6a1-4961-8f21-e355dbca4f88"><div class="source">https://github.com/CoffeeBeforeArch/cuda_programming</div></figure><p id="a819b4a8-64eb-4365-8a31-a8a5e8af3964" class=""><a href="https://www.youtube.com/@CUDAMODE">https://www.youtube.com/@CUDAMODE</a></p><p id="46171282-4814-4197-8445-e8384e844ce4" class=""><a href="https://discord.gg/cudamode">https://discord.gg/cudamode</a></p></details></li></ul><ul id="c42911f5-62ae-43b1-8ce2-ba17725edfd0" class="bulleted-list"><li style="list-style-type:disc">Short AD here before we begin (take this out)</li></ul></details></li></ul><p id="d823fe53-1bb8-475a-8cb2-91b5b3f00968" class="">
</p></div></details><details open=""><summary style="font-weight:600;font-size:1.5em;line-height:1.3;margin:0">Surface Level Overview of the Deep Learning Ecosystem</summary><div class="indented"><p id="8f55fc7e-3f46-464f-93c1-221005f1946b" class="">DISCLAIMER: </p><p id="17c7e491-ee94-4f9f-8c7b-ccb994cc5441" class="">This part doesn’t go over anything highly technical with CUDA. Better to show you the ecosystem rather than enter technical details blindly. From my experience learning this stuff, having a decent enough understanding of the ecosystem will help you map out everything properly, and it provides that initial motivation to learn. </p><p id="d145d32d-7366-4ea3-b7f2-7c49c1a3e1e6" class="">As we mine further into detail, I encourage you to research and play around with what you find interesting (you will come across cool stuff in this section). If you just listen to someone talk about a subject for 20 hours, you limit your learning. Understanding breadth and depth of deep learning infrastructure is tough to navigate. Getting uncomfortable and breaking things is the best way to learn.</p><p id="b8d5970a-d192-41c3-83ef-f4a61bf03416" class="">
</p><ul id="78c433fe-3c70-4556-a60c-41add0da636e" class="toggle"><li><details open=""><summary>Research</summary><ul id="5e84b8d9-7b64-4c36-834b-acc3f5984906" class="toggle"><li><details open=""><summary>PyTorch ( <a href="https://www.youtube.com/watch?v=ORMx45xqWkA&amp;t=13s&amp;ab_channel=Fireship">https://www.youtube.com/watch?v=ORMx45xqWkA&amp;t=13s&amp;ab_channel=Fireship</a>)</summary><ul id="e3099a28-9c8f-42dd-b812-38084ed91027" class="bulleted-list"><li style="list-style-type:disc">If you’re watching this I assume you have at least some basic knowledge of PyTorch. If not, I suggest watching the PyTorch video by Daniel Bourke (<a href="https://www.youtube.com/watch?v=Z_ikDlimN6A">https://www.youtube.com/watch?v=Z_ikDlimN6A</a>)</li></ul><ul id="147747df-20bf-4568-bef8-9d3e145141a2" class="bulleted-list"><li style="list-style-type:disc">Pytorch comes with nightly and stable versions ⇒ <a href="https://discuss.pytorch.org/t/pytorch-nightly-vs-stable/105633">https://discuss.pytorch.org/t/pytorch-nightly-vs-stable/105633</a><p id="6108750d-ed26-4f56-99b2-4966292df0e1" class="">the nightly releases are more likely to be unstable but offer bleeding edge pytorch updates and universal framework optimizations</p></li></ul><ul id="5a74a777-f03b-4d2e-bbc0-3e25907dd12e" class="bulleted-list"><li style="list-style-type:disc">Users prefer pytorch more due to the usability w/ Huggingface</li></ul><ul id="54ad4155-27ea-4ea9-b14f-ec2f5e7b3f04" class="bulleted-list"><li style="list-style-type:disc">You will find pre-trained models on torchvision (<code>pip install torchvision</code>) and <code>torch.hub</code>. The pytorch ecosystem has developed a more decentralized but slightly harder to navigate approach to getting pretrained models. People will often release their models on github repos instead of pushing to a centralized database of models. Huggingface is most commonly used due to community efforts</li></ul><ul id="5f5af12e-a1a8-4655-816b-018f48e512ac" class="bulleted-list"><li style="list-style-type:disc">Good ONNX support</li></ul></details></li></ul><ul id="8226e6c8-0eeb-4dc8-8044-0d36cd1285ec" class="toggle"><li><details open=""><summary>TensorFlow (<a href="https://www.youtube.com/watch?v=i8NETqtGHms">https://www.youtube.com/watch?v=i8NETqtGHms</a>)</summary><ul id="e8adf77b-afbe-463b-b4b6-8d5e8a5bb2b9" class="bulleted-list"><li style="list-style-type:disc">Well documented and lots of community support. Also most used deep learning framework</li></ul><ul id="bc42a337-f580-4a34-835f-f890c870a188" class="bulleted-list"><li style="list-style-type:disc">Comparatively the slowest DL framework</li></ul><ul id="0307020d-2585-4849-a2fc-242228093973" class="bulleted-list"><li style="list-style-type:disc">Created by Google (designed for TPUs) and general purpose ML (SVM, decision trees, etc). </li></ul><ul id="23b22333-e3c4-456e-b6da-96350c49764c" class="bulleted-list"><li style="list-style-type:disc">Pre-trained models can be found directly on ⇒ <a href="https://www.tensorflow.org/resources/models-datasets">https://www.tensorflow.org/resources/models-datasets</a></li></ul><ul id="79c876cc-4a12-4952-bee2-aef61dd23a99" class="bulleted-list"><li style="list-style-type:disc">Good support for pre-trained models download in 1-3 lines of code.</li></ul><ul id="b095086b-7167-4274-a371-fe372215c3c7" class="bulleted-list"><li style="list-style-type:disc">Limited ONNX support (<code>tf2onnx</code>)</li></ul></details></li></ul><ul id="5587fff1-acff-4d4c-99f6-11a138b2f2b5" class="toggle"><li><details open=""><summary>Keras</summary><ul id="0681ba88-677f-4aec-bdbd-b868c954899a" class="bulleted-list"><li style="list-style-type:disc">Similar to <code>torch.nn</code> for TensorFlow, but higher-level. </li></ul><ul id="92ef88fa-e22a-4eae-b796-5e3fbc31a2aa" class="bulleted-list"><li style="list-style-type:disc">Separate library but deeply integrated with TF, serving as its primary high-level API</li></ul><ul id="2adf4fef-981a-4de3-9202-ee583c13c17c" class="bulleted-list"><li style="list-style-type:disc">Complete framework for building and training modules instead of just neural network modules</li></ul></details></li></ul><ul id="61f82975-2f7f-4b51-927c-a2d72e728e8b" class="toggle"><li><details open=""><summary>JAX (<a href="https://www.youtube.com/watch?v=_0D5lXDjNpw">https://www.youtube.com/watch?v=_0D5lXDjNpw</a>)</summary><ul id="6fe926e0-9500-41d9-9576-35bce748b109" class="bulleted-list"><li style="list-style-type:disc">JIT-compiled Autograd Xccelerated Linear Algebra</li></ul><ul id="ca3823da-f769-475d-aeb7-a7f7be41a052" class="bulleted-list"><li style="list-style-type:disc">Docs here ⇒ <a href="https://jax.readthedocs.io/en/latest/">https://jax.readthedocs.io/en/latest/</a></li></ul><ul id="569047a9-507c-44aa-8495-dd0bc9f5d7de" class="bulleted-list"><li style="list-style-type:disc">Feels like numpy</li></ul><ul id="77ce3dae-8e57-42b5-b31b-c74f010726a8" class="bulleted-list"><li style="list-style-type:disc">Reddit sentiment on JAX ⇒ <a href="https://www.reddit.com/r/MachineLearning/comments/1b08qv6/d_is_it_worth_switching_to_jax_from/">https://www.reddit.com/r/MachineLearning/comments/1b08qv6/d_is_it_worth_switching_to_jax_from/</a></li></ul><ul id="d5bcf4a1-d483-4524-8377-e6a08dcc4f0b" class="bulleted-list"><li style="list-style-type:disc">JAX and Tensorflow are both developed by Google</li></ul><ul id="70566913-2f86-41c7-90a7-b7a0957a0d20" class="bulleted-list"><li style="list-style-type:disc">Uses XLA (xccelerated linear algebra) compiler</li></ul><ul id="bcaea11e-4594-4d8f-b579-8fd23ec71cc1" class="bulleted-list"><li style="list-style-type:disc"><code>tf2onnx</code> supported</li></ul></details></li></ul><ul id="ceeaf59c-f5f4-4f15-ab88-602f11d2f813" class="toggle"><li><details open=""><summary>MLX</summary><ul id="19ef1b84-6d5b-4f17-9cbb-d7803fb1f885" class="bulleted-list"><li style="list-style-type:disc">Developed by Apple for Apple Silicon</li></ul><ul id="4a7df49a-935d-4502-9087-800a12e24437" class="bulleted-list"><li style="list-style-type:disc">Open-source framework</li></ul><ul id="eae67919-b148-431b-9719-abe920ab2ecd" class="bulleted-list"><li style="list-style-type:disc">Focuses on high-performance machine learning on Apple devices</li></ul><ul id="6b4209c9-1aae-479a-b055-d3e6966ae979" class="bulleted-list"><li style="list-style-type:disc">Designed for both training and inference</li></ul><ul id="0f397788-3761-4725-b513-22839c46cb34" class="bulleted-list"><li style="list-style-type:disc">Optimized for Apple&#x27;s Metal GPU architecture</li></ul><ul id="b03b28b2-5375-4a68-ada1-939ffc688b78" class="bulleted-list"><li style="list-style-type:disc">Allows for dynamic computation graphs</li></ul><ul id="10c76db3-6a88-41be-811c-65267f01a813" class="bulleted-list"><li style="list-style-type:disc">Suitable for research and development of new ML models</li></ul></details></li></ul><ul id="3453c821-15b4-4095-b967-cf4dee9f6dcc" class="toggle"><li><details open=""><summary>PyTorch Lightning</summary><ul id="eac36799-d63d-44a1-b93e-be8f96453ebc" class="bulleted-list"><li style="list-style-type:disc"><a href="https://www.reddit.com/r/deeplearning/comments/t31ppy/for_what_reason_do_you_or_dont_you_use_pytorch/">https://www.reddit.com/r/deeplearning/comments/t31ppy/for_what_reason_do_you_or_dont_you_use_pytorch/</a></li></ul><ul id="8b5626ae-4e20-41ec-85e0-ff594dbb4a11" class="bulleted-list"><li style="list-style-type:disc">mostly the boilerplate code reduction and distributed scaling</li></ul><ul id="0a57a70d-b9e7-49f9-bd5c-ac4d098fa973" class="bulleted-list"><li style="list-style-type:disc"><code>Trainer()</code> as opposed to training loop</li></ul></details></li></ul></details></li></ul><ul id="98032bf5-0a61-41d0-93f8-fb3248e99ddb" class="toggle"><li><details open=""><summary>Production</summary><ul id="4649e533-f692-489b-9a1e-e459961fcb48" class="toggle"><li><details open=""><summary>Inference-only</summary><ul id="c0ad2c2f-d3de-47d7-a96f-16327a4bbecd" class="toggle"><li><details open=""><summary>vLLM</summary><ul id="b7fde0ef-c396-4bad-92c8-23d486e6b18e" class="bulleted-list"><li style="list-style-type:disc"><a href="https://github.com/vllm-project/vllm">https://github.com/vllm-project/vllm</a></li></ul></details></li></ul><ul id="b346451d-0059-41c0-9771-45d8f66aad8f" class="toggle"><li><details open=""><summary>TensorRT</summary><ul id="27d2af6e-2faa-4717-bbcb-c71cf4949ae6" class="bulleted-list"><li style="list-style-type:disc">integrates well with pytorch for inference</li></ul><ul id="4a680089-99b0-4c7b-ba9b-d83546012bf3" class="bulleted-list"><li style="list-style-type:disc">supports ONNX for loading models</li></ul><ul id="9eab0b61-ec4d-488f-86ea-2c4c13fc65d7" class="bulleted-list"><li style="list-style-type:disc">highly optimized cuda kernels with the following in mind<ul id="8d52d06f-56b4-4016-ae13-335f955e3908" class="bulleted-list"><li style="list-style-type:circle">benefits from sparsity</li></ul><ul id="180c6d26-a960-4cb1-8a7b-6aafc88bcc73" class="bulleted-list"><li style="list-style-type:circle">inference quantization</li></ul><ul id="3457c33a-ddf3-4868-90f3-cdb7d1166d3b" class="bulleted-list"><li style="list-style-type:circle">hardware architecture</li></ul><ul id="c90faf17-c67d-4325-ad1f-956de87db664" class="bulleted-list"><li style="list-style-type:circle">memory access patterns across VRAM vs on-chip memory</li></ul></li></ul><ul id="22d5c2c9-99ff-40ae-af12-ce820fedff85" class="bulleted-list"><li style="list-style-type:disc">short for tensor RunTime</li></ul><ul id="792ec508-0967-43ae-9cd8-cec2db938055" class="bulleted-list"><li style="list-style-type:disc">developed, designed, and maintained by Nvidia</li></ul><ul id="319819a2-4568-4a2a-b6b7-0205e9d48a6a" class="bulleted-list"><li style="list-style-type:disc">built specifically for LLM inference</li></ul><ul id="691de646-eb33-48a6-90ea-62e1aad0857b" class="bulleted-list"><li style="list-style-type:disc">uses some of the techniques we cover in this course, but abstracts them away for usability</li></ul><ul id="55086e0c-e704-43d2-bddc-87709070ef99" class="bulleted-list"><li style="list-style-type:disc">seems that tensorRT requires Onnx look into this</li></ul><ul id="fd973299-bc53-486d-8c8a-fa7f9a41b21f" class="bulleted-list"><li style="list-style-type:disc">review <a href="https://nvidia.github.io/TensorRT-LLM/">https://nvidia.github.io/TensorRT-LLM/</a> vaguely</li></ul><ul id="06eff5a8-fead-468d-af5d-9b51e4204fb2" class="bulleted-list"><li style="list-style-type:disc">follow links in order<ul id="963a2a56-b7a6-47de-b2bc-85195f688cac" class="bulleted-list"><li style="list-style-type:circle"><a href="https://nvidia.github.io/TensorRT-LLM/">https://nvidia.github.io/TensorRT-LLM/</a></li></ul><ul id="bd9a21fc-3937-41ca-bd39-1172c681de7d" class="bulleted-list"><li style="list-style-type:circle"><a href="https://nvidia.github.io/TensorRT-LLM/quick-start-guide.html">https://nvidia.github.io/TensorRT-LLM/quick-start-guide.html</a></li></ul><ul id="2441c087-3f7e-4285-99c3-74108520e577" class="bulleted-list"><li style="list-style-type:circle"><a href="https://pytorch.org/TensorRT/getting_started/installation.html#installation">https://pytorch.org/TensorRT/getting_started/installation.html#installation</a></li></ul></li></ul></details></li></ul><ul id="9d962803-c006-46cc-9dfa-0cc0d147a4fa" class="toggle"><li><details open=""><summary></summary></details></li></ul></details></li></ul><ul id="d6d5118c-f35a-4a21-a141-dc919947a86a" class="toggle"><li><details open=""><summary>Triton</summary><ul id="5e3c65bb-fbe4-4fe6-a4e3-ddaaec8ece20" class="bulleted-list"><li style="list-style-type:disc">Developed and maintained by OpenAI ⇒ <a href="https://openai.com/index/triton/">https://openai.com/index/triton/</a></li></ul><ul id="961f2d09-b5ae-4766-b084-7c87eb08e9d0" class="bulleted-list"><li style="list-style-type:disc"><a href="https://github.com/triton-lang/triton">https://github.com/triton-lang/triton</a></li></ul><ul id="1b565c98-3663-4285-a186-18e1a50b28c6" class="bulleted-list"><li style="list-style-type:disc">CUDA-like, but in python and gets rid of clutter around kernel development in regular CUDA C/C++. Also matches record performance on Matrix Multiplication</li></ul><ul id="b2e2c825-1f1c-49c8-ae16-a56146d2be70" class="bulleted-list"><li style="list-style-type:disc">Get started ⇒ <a href="https://triton-lang.org/main/index.html">https://triton-lang.org/main/index.html</a></li></ul><ul id="731ef764-8a84-4487-8b24-92dc09e17485" class="bulleted-list"><li style="list-style-type:disc">Write your first Triton kernel ⇒ <a href="https://triton-lang.org/main/getting-started/tutorials/index.html">https://triton-lang.org/main/getting-started/tutorials/index.html</a></li></ul><ul id="947bd4b2-94ef-488a-9778-47a37126a2ef" class="toggle"><li><details open=""><summary>Triton Inference Server</summary><ul id="9bb6697a-b854-4800-8242-467c30467b50" class="bulleted-list"><li style="list-style-type:disc"><a href="https://developer.nvidia.com/triton-inference-server">https://developer.nvidia.com/triton-inference-server</a></li></ul><ul id="6ac4590e-e203-4c3b-979c-222adbc293b6" class="bulleted-list"><li style="list-style-type:disc"><a href="https://github.com/triton-inference-server/server">https://github.com/triton-inference-server/server</a></li></ul><ul id="12747154-1da3-4a26-8c56-44e341378dfa" class="bulleted-list"><li style="list-style-type:disc"></li></ul></details></li></ul><ul id="96da7de6-f9d3-4a5c-bc66-1d1edd21ff31" class="bulleted-list"><li style="list-style-type:disc"><a href="https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf">https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf</a> is the original Triton paper</li></ul><ul id="fb0e4f02-4037-4cf7-98b6-3cd78555cc16" class="bulleted-list"><li style="list-style-type:disc"> triton-viz is Triton’s main profiling and visualization toolkit</li></ul><ul id="442a59b1-e226-490f-80f6-b9d1cd5890c5" class="toggle"><li><details open=""><summary>Python to finely control what happens on the GPU without worrying about the unexpected intricacies and complexities in C/C++.</summary><ul id="055d0b65-8825-4b5d-a334-fd7827dee14a" class="bulleted-list"><li style="list-style-type:disc">Removes explicit memory management <code>cudaMalloc</code>, <code>cudaMemcpy</code>, <code>cudaFree</code></li></ul><ul id="5cb5b7f5-4ccb-4ca0-87fe-7cc5c75e2c3d" class="bulleted-list"><li style="list-style-type:disc">No need for error checking / macros <code>CUDA_CHECK_ERROR</code></li></ul><ul id="d4303a24-c1a6-44eb-b18b-c62d29a35a5c" class="bulleted-list"><li style="list-style-type:disc">Reduced complexity when grid / block / thread level indexing on kernel launch parameters</li></ul></details></li></ul><figure id="caad2469-5c36-41a5-8c7a-a0650e24c45e" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled.png"><img style="width:528px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled.png"/></a></figure><p id="cd379500-03f0-4c63-87dc-96361c93da5a" class="">
</p><p id="97d3d65d-8a8a-47f4-8c22-99f60b43b2e1" class="">
</p></details></li></ul><ul id="401cf8de-2d63-4a3c-aa6d-f589298bfaee" class="toggle"><li><details open=""><summary>torch.compile</summary><ul id="01e8cd2a-e8ff-4a4e-9874-a5dde6c6decf" class="bulleted-list"><li style="list-style-type:disc">Gets more attention than TorchScript and is typically better performance</li></ul><ul id="a6b07682-bf9b-46fb-b628-a0f75e4658ee" class="bulleted-list"><li style="list-style-type:disc">Compiles a model down to a static representation so the dynamic graph component of pytorch doesn’t have to worry about things changing. Runs the model as an optimized binary instead of default out-of-the-box pytorch</li></ul><ul id="74a21d1b-0fe6-44fb-ae44-fa9310d78e7c" class="bulleted-list"><li style="list-style-type:disc"><a href="https://discuss.pytorch.org/t/the-difference-between-torch-jit-script-and-torch-compile/188167">https://discuss.pytorch.org/t/the-difference-between-torch-jit-script-and-torch-compile/188167</a></li></ul></details></li></ul><ul id="4af87c31-04a5-448c-b41f-bd7e6249d52a" class="toggle"><li><details open=""><summary>TorchScript</summary><ul id="df14f2e5-6a15-4565-8594-7c132c884d39" class="bulleted-list"><li style="list-style-type:disc">Can be faster in scenarios, especially when deployed in C++</li></ul><ul id="6fcc496f-15ae-49ca-bd31-ffe8a759cf73" class="bulleted-list"><li style="list-style-type:disc">Performance gains can be specific to your neural net architecture</li></ul><ul id="31782d6c-489b-43b5-aed7-7339c7dbe781" class="bulleted-list"><li style="list-style-type:disc"><a href="https://discuss.pytorch.org/t/the-difference-between-torch-jit-script-and-torch-compile/188167">https://discuss.pytorch.org/t/the-difference-between-torch-jit-script-and-torch-compile/188167</a></li></ul></details></li></ul><ul id="1039feb0-fdd0-496d-ac15-24adce50874a" class="toggle"><li><details open=""><summary>ONNX Runtime</summary><ul id="84014745-e200-430a-88ba-230e04e729e4" class="bulleted-list"><li style="list-style-type:disc"><a href="https://youtu.be/M4o4YRVba4o">https://youtu.be/M4o4YRVba4o</a></li></ul><ul id="52c29088-49ae-4a5b-9779-cab277705b02" class="bulleted-list"><li style="list-style-type:disc"><a href="https://github.com/microsoft/onnxruntime">https://github.com/microsoft/onnxruntime</a> ⇒ “<strong>ONNX Runtime training</strong> can accelerate the model training time on multi-node NVIDIA GPUs for transformer models with a one-line addition for existing PyTorch training scripts”</li></ul><ul id="7fc46e15-5c95-48c3-975d-6e8139781d5c" class="bulleted-list"><li style="list-style-type:disc">Developed and maintained by Microsoft</li></ul></details></li></ul><ul id="5330a4a2-99b6-41f6-b9bf-3432e319e2ee" class="toggle"><li><details open=""><summary>Detectron2</summary><ul id="712554a0-2e23-4752-814c-427368d3d66c" class="bulleted-list"><li style="list-style-type:disc"><a href="https://github.com/facebookresearch/detectron2">https://github.com/facebookresearch/detectron2</a></li></ul><ul id="02d00027-e5c4-4de2-9a9b-746dc9f3ad4c" class="bulleted-list"><li style="list-style-type:disc">Supports training and inference</li></ul><ul id="69ffdca3-3ce4-470b-baa1-751a7ea44bf5" class="bulleted-list"><li style="list-style-type:disc">Computer vision project started at Facebook (Meta)</li></ul><ul id="07f19e7c-0dfe-4090-9db4-7a8a6b594ca7" class="bulleted-list"><li style="list-style-type:disc">Detection and segmentation algorithms</li></ul></details></li></ul></details></li></ul><ul id="3de45f64-5a2e-4d1a-88b6-28afbdf2abd6" class="toggle"><li><details open=""><summary>Low-Level</summary><ul id="c4ad4583-fdd8-41e2-852e-ebe25eb86ddd" class="toggle"><li><details open=""><summary>CUDA</summary><ul id="ba817c96-05c0-45f9-92ee-33631a64c5f2" class="bulleted-list"><li style="list-style-type:disc">Compute unified device architecture (CUDA) can be thought of as a programming language for nvidia gpus.</li></ul><ul id="129a565e-fdb7-4916-ba09-4b2a62c99943" class="bulleted-list"><li style="list-style-type:disc">CUDA libs ⇒ cuDNN, cuBLAS, cutlass (fast linear algebra and DL algorithms). cuFFT for fast convolutions (FFTs are covered in the course)</li></ul><ul id="68e3c796-8e58-4b9b-8480-c324b7ef785f" class="bulleted-list"><li style="list-style-type:disc">writing the kernel yourself based on the hardware architecture (Nvidia still does this under the hood for by passing in special flags to the compiler)</li></ul></details></li></ul><ul id="8f9f0019-ad6a-418d-ac19-00eab85bc97e" class="toggle"><li><details open=""><summary>ROCm</summary><ul id="06930345-3d06-4b19-9a8e-50909f41c686" class="bulleted-list"><li style="list-style-type:disc">CUDA equivalent for AMD GPUs</li></ul></details></li></ul><ul id="4be3b994-57d7-4e16-a960-a463f01ca22a" class="toggle"><li><details open=""><summary>OpenCL</summary><ul id="7eca726d-515e-4f99-96c2-d76604057cee" class="bulleted-list"><li style="list-style-type:disc">Open Computing Language</li></ul><ul id="fff98838-fb87-4776-9201-096bd4fb3972" class="bulleted-list"><li style="list-style-type:disc">CPUs, GPUs, digital signal processors, other hardware</li></ul><ul id="d6ab5efa-6d43-4da5-9bbe-79560b0c121f" class="bulleted-list"><li style="list-style-type:disc">since NVIDIA designed CUDA, it will outperform OpenCL on Nvidia tasks. If you are doing work with embedded systems (EE/CE), this is still worth learning. </li></ul></details></li></ul></details></li></ul><ul id="88b775c1-3a8f-41bf-8c4a-5b64f44fe1f1" class="toggle"><li><details open=""><summary>Inference for Edge Computing &amp; Embedded Systems</summary><p id="a6c95f50-c408-4d16-b521-c67d91ffc273" class="">Edge Computing refers to low-latency and highly efficient local computing in the context of real-world distributed systems like fleets. Tesla FSD is a prime example of edge computing because it has a neural net running locally on the car. It also has to send data back to Tesla so they can improve their models. </p><ul id="da05b510-c24b-4453-801b-8292bdee8002" class="toggle"><li><details open=""><summary>CoreML</summary><ul id="540bd742-cef0-41f7-bba2-ec2486e6dcce" class="bulleted-list"><li style="list-style-type:disc">Primarily for deployment of pre-trained models on Apple devices</li></ul><ul id="d3bb7bb9-75e9-4944-9aea-8672d82e231a" class="bulleted-list"><li style="list-style-type:disc">Optimized for on-device inference</li></ul><ul id="29e5648c-6e82-419f-953b-3f0b2cbb135c" class="bulleted-list"><li style="list-style-type:disc">Supports on-device training</li></ul><ul id="80100a48-9c07-4d35-ace9-474cfece42fe" class="bulleted-list"><li style="list-style-type:disc">Supports a wide range of model types (vision, natural language, speech, etc.)</li></ul><ul id="559d8925-d28f-449e-9db1-67ab1bab87a7" class="bulleted-list"><li style="list-style-type:disc">Integrates well with Apple&#x27;s ecosystem (iOS, macOS, watchOS, tvOS)</li></ul><ul id="1a99d565-c8ab-4312-8399-6bb0c4bb48c2" class="bulleted-list"><li style="list-style-type:disc">Focuses on privacy by keeping data on-device</li></ul><ul id="7fabdf82-55b0-48a2-882a-1e33f2bd3b19" class="bulleted-list"><li style="list-style-type:disc">Allows model conversion from other frameworks</li></ul><ul id="6d7f7608-9207-4aff-b77f-6c382f17a833" class="bulleted-list"><li style="list-style-type:disc">Designed for app developers to easily incorporate ML into their apps</li></ul></details></li></ul><ul id="e350fa39-eaa8-4c61-94f1-bb9377bbd33e" class="toggle"><li><details open=""><summary>PyTorch Mobile</summary></details></li></ul><ul id="b8a58f8b-3f5f-44f6-ac86-aa255d5e4c9b" class="toggle"><li><details open=""><summary>TensorFlow Lite</summary></details></li></ul></details></li></ul><ul id="1d919aa7-5d21-4041-bbfb-a05d578b387b" class="toggle"><li><details open=""><summary>Easy to Use</summary><ul id="90fc298a-15c8-4de8-a3eb-4a0cbce4dcf0" class="toggle"><li><details open=""><summary>FastAI</summary><ul id="e65274cc-462c-4e93-8cf4-97c5f6af71c8" class="bulleted-list"><li style="list-style-type:disc">High-level API: Built on top of PyTorch, FastAI provides a more user-friendly interface for common deep learning tasks.</li></ul><ul id="89b33987-6df3-44a2-892e-ee3f9113b8a6" class="bulleted-list"><li style="list-style-type:disc">Rapid prototyping: Designed for quick implementation of state-of-the-art deep learning models.</li></ul><ul id="5e79ce26-2165-498d-affe-40673334e4c7" class="bulleted-list"><li style="list-style-type:disc">Best practices: Incorporates many best practices and recent advances in deep learning by default.</li></ul><ul id="0f16a26c-fdb2-451d-ae20-8a00e3c714a8" class="bulleted-list"><li style="list-style-type:disc">Less code: Typically requires less code to implement complex models compared to raw PyTorch.</li></ul><ul id="52beb73f-b699-4d76-9bd7-bc46b5f89234" class="bulleted-list"><li style="list-style-type:disc">Transfer learning: Excellent support for transfer learning out of the box.</li></ul></details></li></ul><ul id="76fa1b5f-3836-4925-990e-c27b4a6d7b79" class="toggle"><li><details open=""><summary>ONNX </summary><ul id="35490ced-69bb-4f31-8e1a-87bcd0b608c8" class="bulleted-list"><li style="list-style-type:disc">Open Neural Network eXchange</li></ul><ul id="35962186-5ea3-4878-b6d8-91b1a320cc9d" class="bulleted-list"><li style="list-style-type:disc"><code>torch.onnx.export(model, dummy_input, &quot;resnet18.onnx&quot;)</code></li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="85f53152-3e85-481a-b4c2-8b7422bf1094" class="code"><code class="language-Python">import tensorflow as tf
import tf2onnx
import onnx

# Load your TensorFlow model
tf_model = tf.keras.models.load_model(&#x27;path/to/your/model.h5&#x27;)

# Convert the model to ONNX
onnx_model, _ = tf2onnx.convert.from_keras(tf_model)

# Save the ONNX model
onnx.save(onnx_model, &#x27;path/to/save/model.onnx&#x27;)</code></pre><figure id="e712aa00-1022-4dbb-8393-b03956461646" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%201.png"><img style="width:336px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%201.png"/></a></figure></details></li></ul><ul id="6dc302ca-6256-4b99-abd6-b265d3d42b77" class="toggle"><li><details open=""><summary>wandb</summary><ul id="4bfab188-1369-405e-91b0-bb2da4e14c57" class="bulleted-list"><li style="list-style-type:disc">Short for weights and biases</li></ul><ul id="b674189e-5466-4745-8cd8-a42ad9cddae3" class="bulleted-list"><li style="list-style-type:disc">Easy to integrate with projects w/ a few lines of code</li></ul><ul id="c53f50bf-eb6a-44b6-891c-9602abac9973" class="bulleted-list"><li style="list-style-type:disc">Team collaboration</li></ul><ul id="f8f062d1-1ccb-487b-97f9-288fccfb2950" class="bulleted-list"><li style="list-style-type:disc">Compare experiments w/ an intuitive UI</li></ul><figure id="9f1fa355-db78-47da-8d0c-8181418edf74" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%202.png"><img style="width:576px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%202.png"/></a></figure></details></li></ul><p id="d7e202aa-73a8-4fca-a6f0-d5782709aee6" class="">
</p></details></li></ul><ul id="80baef82-f5d8-489d-b4d4-3a0144a0da03" class="toggle"><li><details open=""><summary>Cloud Providers</summary><ul id="733f8d9e-aa62-4b2a-83b2-787a079ad725" class="toggle"><li><details open=""><summary>AWS</summary><ul id="a3922ffa-7f58-4479-bd9b-3580c1540cbf" class="bulleted-list"><li style="list-style-type:disc">EC2 instances</li></ul><ul id="09751e0b-3599-4a11-b133-ad9b13f94ec8" class="bulleted-list"><li style="list-style-type:disc">Sagemaker (jupyter notebooks on a cluster, human data labelling/annotation, model training &amp; deployment on AWS infrastructure)</li></ul></details></li></ul><ul id="f38813bf-3776-4d80-a318-d29f8e2b8e9e" class="toggle"><li><details open=""><summary>Google Cloud</summary><ul id="6b8317bf-bee6-4b45-a704-678a55073351" class="bulleted-list"><li style="list-style-type:disc">Vertex AI</li></ul><ul id="5669bd75-be9c-4658-a0d2-c48852d182b7" class="bulleted-list"><li style="list-style-type:disc">VM instances</li></ul></details></li></ul><ul id="55ac5227-ac03-45cb-82de-60ca9f5ce589" class="toggle"><li><details open=""><summary>Microsoft Azure</summary><ul id="70da8857-adb4-42b6-9a68-a31a16008128" class="bulleted-list"><li style="list-style-type:disc">Deep speed</li></ul></details></li></ul><ul id="e8b97216-c623-43be-a975-1b4e3794ebf1" class="toggle"><li><details open=""><summary>OpenAI</summary></details></li></ul><ul id="0d932358-6e9a-4194-86bc-1dff88810cba" class="toggle"><li><details open=""><summary>VastAI</summary><ul id="3d8650b7-8aa9-43be-80af-9d564e163b8a" class="bulleted-list"><li style="list-style-type:disc">link picture of UI here</li></ul></details></li></ul><ul id="ece426ba-c2a6-4003-ad46-96127de6a53f" class="toggle"><li><details open=""><summary>Lambda Labs</summary><ul id="609535b3-089f-406d-b4e0-af921b652369" class="bulleted-list"><li style="list-style-type:disc">Cheap datacenter GPUs</li></ul></details></li></ul></details></li></ul><ul id="1c9aaf2e-1068-4947-b078-ea36828827ed" class="toggle"><li><details open=""><summary>Compilers</summary><ul id="35dd3b97-ec8a-4373-8003-b46d3f020ffd" class="toggle"><li><details open=""><summary>XLA</summary><ul id="16f5e1a2-1599-435b-9455-7f31d833a6dc" class="bulleted-list"><li style="list-style-type:disc">A domain-specific compiler for linear algebra that optimizes TensorFlow computations</li></ul><ul id="b3a4407c-2930-4642-a633-b3881f46d512" class="bulleted-list"><li style="list-style-type:disc">Provides a lower-level optimization and code generation backend for JAX</li></ul><ul id="1547efda-cec1-4c6a-a32e-fbe41e46a5e7" class="bulleted-list"><li style="list-style-type:disc">Performs whole-program optimization, seeing beyond individual operations to optimize across the entire computation graph</li></ul><ul id="b1dc4a78-8ae3-4575-bc64-4b379877693c" class="bulleted-list"><li style="list-style-type:disc">Enables efficient execution on various hardware (CPUs, GPUs, TPUs) by generating optimized machine code</li></ul><ul id="2aaeac44-a39a-4bfb-baee-3e30a2b9d890" class="bulleted-list"><li style="list-style-type:disc">Implements advanced optimizations like operation fusion, which combines multiple operations into a single, more efficient kernel</li></ul><ul id="c7ce83bc-0450-4780-acf8-a0a9251cf945" class="bulleted-list"><li style="list-style-type:disc">Allows JAX to achieve high performance without manually writing hardware-specific code</li></ul></details></li></ul><ul id="e9d87167-c08d-4498-9e83-df3fa60d361e" class="toggle"><li><details open=""><summary>LLVM</summary></details></li></ul><ul id="33a098fb-43b4-48cd-8a8d-41c9c1337983" class="toggle"><li><details open=""><summary>MLIR</summary></details></li></ul><ul id="951191a0-40f7-4ebe-8e6d-8a6b78932281" class="toggle"><li><details open=""><summary>NVCC</summary><ul id="37fb473a-211c-456c-86ef-41837f4da46f" class="bulleted-list"><li style="list-style-type:disc">Nvidia CUDA Compiler</li></ul><ul id="76747f2a-2a31-49dd-b6ad-d67318a7f332" class="bulleted-list"><li style="list-style-type:disc">Works on everything in the CUDA toolkit</li></ul><figure id="aea6ec0a-0363-45c0-a46a-7bf9c94b7ae7" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%203.png"><img style="width:384px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%203.png"/></a></figure></details></li></ul></details></li></ul><ul id="642153dd-8af1-4634-b370-9ca205359acb" class="toggle"><li><details open=""><summary>Misc</summary><ul id="43ce8da3-c68b-463a-875a-7a5515442d7e" class="toggle"><li><details open=""><summary>Huggingface</summary></details></li></ul><ul id="8fba7990-acbe-4351-afaf-11698608de91" class="toggle"><li><details open=""><summary></summary></details></li></ul></details></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.5em;line-height:1.3;margin:0">Setup</summary><div class="indented"><ul id="63f4f0ac-7c46-4b16-9433-01def66adbe8" class="bulleted-list"><li style="list-style-type:disc">Windows users<ul id="ab6b22bc-7b4c-454b-8e1e-5119565e72b3" class="bulleted-list"><li style="list-style-type:circle">either wsl2 (windows subsystem for linux)</li></ul><ul id="6b1ec3c1-5d21-41b8-aa0b-7b4d9f0daa6b" class="bulleted-list"><li style="list-style-type:circle">or an isolated docker container w/ a linux environment. you may want to touch up on docker if you’re taking this route. docker is a very useful tool in software development. some common commands you’ll need to use are copying (from container to windows) and saving progress. the last thing you want is to be halfway through the course and find out your progress didn’t save when you exited the docker container.</li></ul></li></ul><ul id="9e07b431-bda8-4481-9352-59efa1eb35a8" class="bulleted-list"><li style="list-style-type:disc">make</li></ul><ul id="3ee63912-35b1-4c5c-96a7-4e6f02f4b6d7" class="bulleted-list"><li style="list-style-type:disc">cmake</li></ul><ul id="ee2fa908-c032-4c4b-9f68-02afc5de2242" class="bulleted-list"><li style="list-style-type:disc">python 3.8 or higher</li></ul><ul id="0f21bf17-a2eb-4078-9c74-c2a2293850b7" class="bulleted-list"><li style="list-style-type:disc">CUDA itself is not a language, but an extension to C and C++ for GPU control.</li></ul><ul id="883e9787-edf0-45c7-a371-2fcc5faf3711" class="bulleted-list"><li style="list-style-type:disc">cuda toolkit (all CUDA libs) &amp; <code>nvcc</code> compiler<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="64b1ea6e-48c8-425b-b7de-c341a075ba08" class="code"><code class="language-Plain Text">wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pinsudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda-repo-ubuntu2204-12-4-local_12.4.0-550.54.14-1_amd64.debsudo dpkg -i cuda-repo-ubuntu2204-12-4-local_12.4.0-550.54.14-1_amd64.debsudo cp /var/cuda-repo-ubuntu2204-12-4-local/cuda-*-keyring.gpg /usr/share/keyrings/sudo apt-get updatesudo apt-get -y install cuda-toolkit-12-4`</code></pre><ul id="42e1941a-0144-4f51-ae1f-6313b51e5c55" class="bulleted-list"><li style="list-style-type:circle">“but it says nvcc not installed” ⇒ <a href="https://askubuntu.com/questions/885610/nvcc-version-command-says-nvcc-is-not-installed">https://askubuntu.com/questions/885610/nvcc-version-command-says-nvcc-is-not-installed</a></li></ul><ul id="0e567a29-cc24-4d0f-b2a4-ade08b55c12a" class="bulleted-list"><li style="list-style-type:circle">then <code>source ~/.zshrc</code> or <code>source ~/.bashrc</code> </li></ul><ul id="ef823599-1893-4f3d-8b32-e76a460321e1" class="bulleted-list"><li style="list-style-type:circle"><a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#">https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#</a></li></ul></li></ul><ul id="c61cd894-c01f-4b52-bc4c-c1f27a74f910" class="bulleted-list"><li style="list-style-type:disc">we are downloading the <code>.so</code> (shared object file ⇒ only for linux)  which contains the functionality for the CUDA libraries. we will use these abstract libraries once the complexity in manual GPU kernel are hammered out. if you’re on windows, you’ll use the <code>.dll</code> files <ul id="53e4f500-a4df-48a1-a7a9-06df7d437301" class="bulleted-list"><li style="list-style-type:circle">compiler properties<ul id="dfde10f4-2b4c-4d1c-9660-88a372a62a4d" class="bulleted-list"><li style="list-style-type:square">LLVM based</li></ul><ul id="b49e8103-fc4d-4652-874b-8fac8fa0bc29" class="bulleted-list"><li style="list-style-type:square">more…</li></ul></li></ul></li></ul><ul id="04185e98-d760-405e-aeb0-8bee8601f0e4" class="toggle"><li><details open=""><summary>GPU stats</summary><ul id="2b7b3c60-a981-4ad5-906d-0c47cbaa054e" class="bulleted-list"><li style="list-style-type:disc"><code>git clone </code><code><a href="https://github.com/NVIDIA/cuda-samples.git">https://github.com/NVIDIA/cuda-samples.git</a></code></li></ul><ul id="9baabeea-67b7-46ee-8204-8f59380d7187" class="bulleted-list"><li style="list-style-type:disc">cd into deviceQuery</li></ul><ul id="818c44c1-3777-4613-8f86-5193098e57ac" class="bulleted-list"><li style="list-style-type:disc">run <code>./deviceQuery</code></li></ul><figure id="7fa943fc-ef1e-4851-8179-f85f7ea0ffcf" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%204.png"><img style="width:949px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%204.png"/></a></figure><ul id="3f5e0efd-19d4-4c51-85da-b71479d3a52d" class="bulleted-list"><li style="list-style-type:disc">we will need these stats later for debugging &amp; optimizing</li></ul><ul id="d2673581-4b5a-4f41-931e-8466128c60ba" class="bulleted-list"><li style="list-style-type:disc">its also fun to look at because now you know everything about your GPU</li></ul><ul id="9f98a0aa-51c1-4171-b5d1-ea2838406c36" class="bulleted-list"><li style="list-style-type:disc">the compute capability (2nd from top) says 8.6 for me which means I can past in that number and the hardware will optimize for my architecture at compile time <code>nvcc -lineinfo -arch=sm_86 -o main main.cu</code>. These are just my notes for knowing whats what when I navigate the course material so no need to focus on these command line arguments early on</li></ul><ul id="a3a53a37-6e3b-4e58-a8b4-d3099da4ea96" class="bulleted-list"><li style="list-style-type:disc">compute capability ⇒ <a href="https://developer.nvidia.com/cuda-gpus">https://developer.nvidia.com/cuda-gpus</a>’</li></ul></details></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.5em;line-height:1.3;margin:0">Overview of C/C++</summary><div class="indented"><ul id="d0110b57-90e1-40c0-8c8a-12a1c6f7f125" class="bulleted-list"><li style="list-style-type:disc">assume people already know the basics but provide links for a refresher</li></ul><ul id="8038313b-04fb-4e96-ad38-732942caf0a1" class="bulleted-list"><li style="list-style-type:disc">C ⇒ <a href="https://www.w3schools.com/c/">https://www.w3schools.com/c/</a></li></ul><ul id="811555c1-6643-4678-985c-8a72ed34f1d5" class="bulleted-list"><li style="list-style-type:disc">C++ ⇒ <a href="https://www.w3schools.com/cpp/default.asp">https://www.w3schools.com/cpp/default.asp</a></li></ul><ul id="f916fb7b-a71d-4551-92d4-b7cca49f0ae8" class="bulleted-list"><li style="list-style-type:disc">recursion refresher</li></ul><ul id="a54bd6f2-7fa5-4e9e-908b-0dc353196f94" class="bulleted-list"><li style="list-style-type:disc">Pointers in C</li></ul><ul id="c331e64a-edd2-47c5-a59f-32400bfaf57d" class="bulleted-list"><li style="list-style-type:disc">Pointers to Pointers to Poi…</li></ul><ul id="82d9b87e-8f4d-4e3d-9c8a-374178ded60c" class="bulleted-list"><li style="list-style-type:disc"><code>void</code> pointers</li></ul><ul id="dbf16d71-56e0-4d8d-845b-92a905a54201" class="bulleted-list"><li style="list-style-type:disc">enums</li></ul><ul id="f77212f1-b868-4281-af28-45498d43c0fe" class="bulleted-list"><li style="list-style-type:disc">file IO</li></ul><ul id="c7cdd74a-c0d5-4399-80f7-b611f67d62e5" class="bulleted-list"><li style="list-style-type:disc">C/C++ compiler</li></ul><ul id="807a1cb3-9821-47a6-8931-9bcc59bf1777" class="bulleted-list"><li style="list-style-type:disc">Advantages posed by C++ compared to C</li></ul><ul id="76bb3706-cff2-4c14-9bd2-0f276049df0b" class="bulleted-list"><li style="list-style-type:disc">Extra types ⇒ <code>size_t</code> is just an <code>unsigned int64</code> designed to hold the size of any array/data structure without getting overloaded (int16 maxes out at 65536)</li></ul><ul id="9b404506-4f79-46f6-971d-19c820051a82" class="bulleted-list"><li style="list-style-type:disc">Types of Type Casting?</li></ul><ul id="8d033f76-03d3-4ca6-8280-ab43e3f20da9" class="bulleted-list"><li style="list-style-type:disc">Macros ⇒ <code>#define</code> and <code>#undef</code></li></ul><ul id="a722addb-41d6-4f38-859d-059abca85e47" class="bulleted-list"><li style="list-style-type:disc">Writing Makefiles</li></ul><ul id="bd1dd152-21aa-4a7a-9c45-e3171a7cbda3" class="bulleted-list"><li style="list-style-type:disc">the heap vs dynamic memory (heap for hyperparams &amp; loss, DRAM for tensors)</li></ul><ul id="d5d98c3c-42e2-40eb-bc1a-7d88d512d168" class="bulleted-list"><li style="list-style-type:disc">C/C++ Videos and Other Resources</li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.5em;line-height:1.3;margin:0">Gentle Introduction to GPUs</summary><div class="indented"><ul id="28ba7201-6421-43ee-be70-4e3bd120c04b" class="bulleted-list"><li style="list-style-type:disc">Okay..but what is CUDA? ⇒ <a href="https://docs.nvidia.com/cuda/">https://docs.nvidia.com/cuda/</a></li></ul><ul id="53601e50-e250-48e0-98d5-ddebf5ad33c7" class="bulleted-list"><li style="list-style-type:disc">Why do we write CUDA kernels?</li></ul><ul id="2b0ceab8-662e-4539-97a2-22b602b3df3b" class="bulleted-list"><li style="list-style-type:disc">GPUs vs TPUs vs FPGAs</li></ul><ul id="ea7ad577-ed45-43e1-8289-8640aa4f5044" class="bulleted-list"><li style="list-style-type:disc">This aims you give you a bit of history about the GPU itself, why we use it for deep learning tasks, why its way faster than the CPU at certain tasks. ex: H100 ⇒ Grace Hopper and contributions<ul id="e088f388-6888-4bd7-b9f3-b1115a44b21f" class="bulleted-list"><li style="list-style-type:circle">Tesla</li></ul><ul id="5161bb1e-6ca8-45af-bc44-a9f9df5bb71e" class="bulleted-list"><li style="list-style-type:circle">Ada</li></ul><ul id="624023b9-6c63-4971-a6f6-ec3995c7381e" class="bulleted-list"><li style="list-style-type:circle">Hopper</li></ul><ul id="c07a2058-71ce-4767-bd66-621d1014c2c4" class="bulleted-list"><li style="list-style-type:circle">Ampere</li></ul><ul id="4c810399-549e-4966-b1c7-0478a3fd36f4" class="bulleted-list"><li style="list-style-type:circle">Turing</li></ul><ul id="38fb3d28-6885-4258-b751-9fadcef2324e" class="bulleted-list"><li style="list-style-type:circle">…</li></ul></li></ul><ul id="9a1cb531-2eff-41a7-96f7-916b6708661a" class="bulleted-list"><li style="list-style-type:disc">Start off with an example of a parallelizable for loop without code. convert the wrapping for loop to an instruction for a bunch of threads at different indices</li></ul><ul id="cb5ed56d-034a-4b15-9dcb-e22fe5440293" class="bulleted-list"><li style="list-style-type:disc">Two most important things are to implement/use the fastest algorithm we can and be able to profile them as we develop (capture bottlenecks and compare your code to a deep learning framework like PyTorch </li></ul><ul id="ebbaee32-cf41-4f3c-b8cd-30c4d67d5703" class="toggle"><li><details open=""><summary><em><strong>Continue w/ notes from all youtube lectures</strong></em></summary><ul id="0ef2f552-0073-4ef1-85d5-b399bc1b6e37" class="bulleted-list"><li style="list-style-type:disc">talk about CPUs vs GPUs</li></ul><ul id="a34d4fcf-1fc3-4168-b084-dea8f1e2baee" class="bulleted-list"><li style="list-style-type:disc">show cpu vs gpu hardware architecture<figure id="30b16556-69c7-4238-89e9-a1e0d8acca8a" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%205.png"><img style="width:1612px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%205.png"/></a></figure></li></ul><ul id="c09fdd8e-27a5-49a0-a154-a6eb9fbf7a4e" class="bulleted-list"><li style="list-style-type:disc">latency (CPU)<ul id="8078fc43-0bbf-45f3-be82-0045ef23a070" class="bulleted-list"><li style="list-style-type:circle">minimize time of one task</li></ul><ul id="96c0fcb2-4ae0-4114-bb38-f8d043ccf48b" class="bulleted-list"><li style="list-style-type:circle">metric: seconds</li></ul></li></ul><ul id="1f7e79c5-52ed-41e4-9c4e-e887f5ca8357" class="bulleted-list"><li style="list-style-type:disc">throughput (GPU)<ul id="c9e326a4-d707-4904-a3c8-2d0067cad46a" class="bulleted-list"><li style="list-style-type:circle">maximize stuff per time</li></ul><ul id="e00b4638-3b1a-41f3-8508-102ddb3351fb" class="bulleted-list"><li style="list-style-type:circle">metric: jobs/hour, pixels/s</li></ul></li></ul><ul id="8dde411a-4c4f-4a83-b5ed-105322064815" class="bulleted-list"><li style="list-style-type:disc">CPU (host) ⇒ functions</li></ul><ul id="1f51df9e-bd4b-4bab-b4bd-100069ac49f2" class="bulleted-list"><li style="list-style-type:disc">GPU (device) ⇒ kernels</li></ul><ul id="7e86d6aa-6cb9-40c0-8cbf-426f80e83232" class="bulleted-list"><li style="list-style-type:disc">The CPU is in charge of setting up the binaries for the GPU ⇒ “host launches kernels on device”</li></ul><ul id="1a92f34d-3da9-4e19-975a-ac04401335ed" class="bulleted-list"><li style="list-style-type:disc">Typical CUDA program<ol type="1" id="ce00125a-4bf0-45fd-81b6-b0c754e8bd02" class="numbered-list" start="1"><li>CPU allocates CPU memory</li></ol><ol type="1" id="fc44b79a-619c-4fe3-b7e7-a5f0743497dd" class="numbered-list" start="2"><li>CPU copies data to GPU</li></ol><ol type="1" id="59396b26-79fd-4d4f-bd5d-87510c4c6602" class="numbered-list" start="3"><li>CPU launches kernel on GPU (processing is done here)</li></ol><ol type="1" id="80a3621a-7978-41fc-9e27-6a7a1e477e87" class="numbered-list" start="4"><li>CPU copies results from GPU back to CPU to do something useful with it</li></ol></li></ul><ul id="3eca6084-43a9-4a2f-a6f1-babc49541f4a" class="bulleted-list"><li style="list-style-type:disc">Kernel looks like a serial program; says nothing about parallelism. Imagine you are trying to solve a jigsaw puzzle (show image) and all you are given is the location of each puzzle piece. The high level algorithm would be designed to take these individual pieces, and solve a single problem for each of them; “put the piece in the correct spot”. As long as all the pieces are assembled in the right place at the end, it works!</li></ul><ul id="9cee2670-1fc2-43df-aadf-f97fac40c9a3" class="bulleted-list"><li style="list-style-type:disc">CPU C code</li></ul><figure id="cd0a98a7-db48-4b4c-9631-261f19b3c7d1" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%206.png"><img style="width:2366px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%206.png"/></a></figure><ul id="56ba3585-7a10-4a20-ad30-268b639ec753" class="bulleted-list"><li style="list-style-type:disc">GPU CUDA code</li></ul><figure id="fd63e5a8-d298-4d6b-bd56-43cb308357e9" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%207.png"><img style="width:1200px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%207.png"/></a></figure></details></li></ul><figure id="630c515d-f48e-4a5f-8224-d7977a83aa3a" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%208.png"><img style="width:1166px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%208.png"/></a></figure><figure id="9ea40c30-047f-48a6-800a-5c566612db23" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%209.png"><img style="width:639.0104370117188px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%209.png"/></a></figure><figure id="29b88783-f6a8-4576-b993-df131952be2f" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2010.png"><img style="width:2918px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2010.png"/></a></figure><ul id="ceb366b5-1615-43a0-bd57-6c57191b2058" class="toggle"><li><details open=""><summary>Architecture Timeline w/ Performance Stats</summary><ul id="46571c66-3d2d-4eba-8d23-39595898ab5c" class="bulleted-list"><li style="list-style-type:disc"><a href="https://www.youtube.com/watch?v=kUqkOAU84bA">https://www.youtube.com/watch?v=kUqkOAU84bA</a> gets very indepth with the CUDA hardware in each architecture</li></ul><figure id="871c2331-086b-4395-9b98-340d9a85ea44" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2011.png"><img style="width:1714px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2011.png"/></a></figure><figure id="728ae4aa-63b3-456b-91d2-5ecf8bc6dd95" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2012.png"><img style="width:1282px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2012.png"/></a></figure><figure id="6044f715-9024-4c38-b95b-768db45978d1" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2013.png"><img style="width:1260px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2013.png"/></a></figure></details></li></ul><ul id="8b2eb660-c5fc-4dd5-93f2-fe81fcdd324f" class="toggle"><li><details open=""><summary>Some terms to remember as we advanced through building GPU kernels</summary><ul id="586e3334-ba75-4fdb-a87a-82d7eab19d70" class="bulleted-list"><li style="list-style-type:disc">kernel<ul id="2561d654-402f-4ac7-915a-39267de99841" class="bulleted-list"><li style="list-style-type:circle">convolution kernels (sliding window kernel)</li></ul><ul id="0a2e3ac8-a6ac-4190-b602-d33381fe3d09" class="bulleted-list"><li style="list-style-type:circle">The core of the Linux system is the kernel. The kernel controls all the hardware and software on the computer system, allocating hardware when necessary and executing software<br/>when required.<br/></li></ul><ul id="8e4cda51-0f6c-4754-ad23-c60e7f0afa6b" class="bulleted-list"><li style="list-style-type:circle">a function we run on the GPU</li></ul><ul id="76d68bc8-8ec4-45f6-bc34-9776e196d0be" class="bulleted-list"><li style="list-style-type:circle">…</li></ul></li></ul><ul id="fc8c31ce-3640-4b8f-a40a-25edb2bd9a7f" class="bulleted-list"><li style="list-style-type:disc">GEMM (GEneral Matrix Multiplication)</li></ul><ul id="00c01b0b-47e4-4f7c-a01d-e0c01d9ef53b" class="bulleted-list"><li style="list-style-type:disc">memory bandwidth (the data transfer speed with respect to different types of memory). shared memory has a way higher memory bandwidth than global memory due to the hardware configuration.  </li></ul><ul id="dbf962a4-6637-4938-92b8-0f65466381f0" class="bulleted-list"><li style="list-style-type:disc">…</li></ul></details></li></ul><ul id="27beec2b-17e2-4dc0-9f45-88dccc233cf8" class="toggle"><li><details open=""><summary>Other</summary><ul id="232ebb40-e879-46b9-88f3-5a0e0c7ec762" class="bulleted-list"><li style="list-style-type:disc">SXM ⇒ Scalable eXtended Matrix ⇒ multi-node (datacenter memory bandwidth)</li></ul><ul id="27a230c0-d044-41b8-866b-c2618e3f9467" class="bulleted-list"><li style="list-style-type:disc">PCIe ⇒ Peripheral Component Interconnect Express (show picture of motherboard slots)</li></ul></details></li></ul><p id="1e6720f7-a418-4a27-be69-899acce1cdb9" class="">
</p></div></details><details open=""><summary style="font-weight:600;font-size:1.5em;line-height:1.3;margin:0">Writing your first CUDA kernels</summary><div class="indented"><p id="e46cd924-a100-430e-b66a-f616e0b86100" class="">include a vector addition CUDA kernel for initial exposure (small code snippet)</p><ul id="8ed42a11-95c0-4f69-a374-8d77520f6a0f" class="toggle"><li><details open=""><summary>General Intuition</summary><figure id="f7c243bf-fb0c-491e-ad07-76bde28499a1" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2014.png"><img style="width:528px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2014.png"/></a></figure><figure id="deedd52b-88d4-430a-9d55-110afa7e546b" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2015.png"><img style="width:2856px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2015.png"/></a></figure><figure id="3e849739-810b-43da-a921-5ccde37dec14" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2016.png"><img style="width:639.0104370117188px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2016.png"/></a></figure></details></li></ul><ul id="38b0edc9-8034-4f52-82b8-1db33e8e0ab1" class="bulleted-list"><li style="list-style-type:disc"><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#</a></li></ul><ul id="5220855b-6ac2-4299-b306-714b14431e6e" class="bulleted-list"><li style="list-style-type:disc">The best intro I’ve found ⇒ <a href="https://developer.nvidia.com/blog/even-easier-introduction-cuda/">https://developer.nvidia.com/blog/even-easier-introduction-cuda/</a></li></ul><ul id="a8e7f9f4-bea5-4936-8792-ff451345b7a2" class="toggle"><li><details open=""><summary>Run your first CUDA kernel (hello world ⇒ printing out thread/block/grid idx’s ⇒ vector addition ⇒ batched matmul)</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="3bff6afe-1535-4c83-a889-e5b33cd7f69e" class="code"><code class="language-C">#include &lt;iostream&gt;
#include &lt;cuda_runtime.h&gt;

// CUDA kernel to perform element-wise vector addition
__global__ void vecAdd(const float* A, const float* B, float* C, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &lt; N) {
        C[i] = A[i] + B[i];
    }
}

int main() {
    // Size of vectors
    int N = 1 &lt;&lt; 20;  // For example, 2^20 elements
    size_t size = N * sizeof(float);

    // Allocate host memory
    float* h_A = (float*)malloc(size);
    float* h_B = (float*)malloc(size);
    float* h_C = (float*)malloc(size);

    // Initialize host vectors
    for (int i = 0; i &lt; N; i++) {
        h_A[i] = static_cast&lt;float&gt;(i);
        h_B[i] = static_cast&lt;float&gt;(i * 2);
    }

    // Allocate device memory
    float *d_A, *d_B, *d_C;
    cudaMalloc((void**)&amp;d_A, size);
    cudaMalloc((void**)&amp;d_B, size);
    cudaMalloc((void**)&amp;d_C, size);

    // Copy vectors from host memory to device memory
    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);

    // Define the number of threads in a block
    int blockSize = 256;
    // Define the number of blocks in a grid
    int numBlocks = (N + blockSize - 1) / blockSize;

    // Launch the vecAdd CUDA kernel
    vecAdd&lt;&lt;&lt;numBlocks, blockSize&gt;&gt;&gt;(d_A, d_B, d_C, N);

    // Copy the result vector from device memory to host memory
    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);

    // Verify the result
    bool success = true;
    for (int i = 0; i &lt; N; i++) {
        if (h_C[i] != h_A[i] + h_B[i]) {
            std::cerr &lt;&lt; &quot;Error at index &quot; &lt;&lt; i &lt;&lt; &quot;: &quot; &lt;&lt;
                h_C[i] &lt;&lt; &quot; != &quot; &lt;&lt; h_A[i] &lt;&lt; &quot; + &quot; &lt;&lt; h_B[i] &lt;&lt; std::endl;
            success = false;
            break;
        }
    }
    if (success) {
        std::cout &lt;&lt; &quot;Vector addition successful!&quot; &lt;&lt; std::endl;
    }

    // Free device memory
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);

    // Free host memory
    free(h_A);
    free(h_B);
    free(h_C);

    return 0;
}</code></pre></details></li></ul><ul id="3743c3a7-109d-4cf8-9fa1-b2ce8f2506e9" class="toggle"><li><details open=""><summary><em><strong>Concepts</strong></em></summary><p id="269c2807-359d-43fc-bd97-cde14547e376" class="">Host ⇒ CPU ⇒ Uses RAM sticks on the motherboard</p><p id="ee8ce70b-9bbb-4464-8f2d-4380dcbc6808" class="">Device ⇒ GPU ⇒ Uses on Chip VRAM (video memory for desktop PCs)</p><p id="79a90e64-0caa-4b25-a61f-839dab41b23a" class="">CUDA program surface level runtime:</p><ol type="1" id="fc97bb4f-18fb-40ea-aa90-ca295a1db804" class="numbered-list" start="1"><li>copy input from host to device</li></ol><ol type="1" id="b17be82a-3a49-4348-af0b-883127e5b6cb" class="numbered-list" start="2"><li>load GPU program and execute using the transferred on-chip data</li></ol><ol type="1" id="b63ae558-4d4f-4920-a443-6d2d22f2571f" class="numbered-list" start="3"><li>copy results from device back to host so you can display/use it somehow</li></ol><p id="36a32375-1e11-431d-b7a3-32af0e12682a" class="">
</p><p id="47b64e76-3225-4ad0-947d-40cfa142d3cd" class=""><code>nvcc</code> compiler</p><p id="5d7f0dd7-7ac0-4d83-90d0-e6389a94e528" class="">off-line compilation<div class="indented"><p id="8c3561fa-c560-4403-86a4-72a0fa0e629c" class="">host code<div class="indented"><p id="a5ad3995-a54f-4bac-8ec1-e2b1e89810ff" class="">modifed to run kernels</p><p id="949c8fdc-47af-4ac5-9ac8-25e3ccfcb515" class="">compiled to x86 binary</p></div></p><p id="0386da3e-0ff9-480b-b9aa-a09e09591951" class="">device code<div class="indented"><p id="7405822a-3c4c-4794-9843-f4bb1736f5a5" class="">compiled to PTX (parallel thread execution)</p><p id="f73157b0-012b-49a4-a497-038d7fc7670b" class="">stable across multiple GPU generations</p></div></p></div></p><p id="22e80e9d-005a-4038-9958-df61f84220d3" class="">JIT (just-in-time)<div class="indented"><p id="c25fe001-31d7-40a7-bfe6-1d3a8a9f402d" class="">PTX into native GPU instructions</p><p id="d346a46b-021d-4240-a7f3-1a3a2fc2cd64" class="">allows for forward compatibility</p></div></p><p id="3f1fa3f8-e7d0-4f40-9c7a-5da271c32b0b" class="">
</p><p id="b32076ff-d05f-4fcd-ac92-6da06111815a" class="">What is a kernel?<div class="indented"><p id="b5d73d0f-d055-434c-88a4-9447ad073305" class="">function executed on GPU in parallel</p><p id="c6b80aad-7668-4303-bb25-3d5bb6d07d21" class="">C function</p><p id="a09bf13c-03d4-4fce-9700-00c81139fb11" class="">executed N times, once on each of N CUDA threads</p><p id="accb2f4e-5bfa-4b0b-80f8-cad9e4106c47" class="">declared using <code>__global__</code></p><p id="b65154e6-e668-4156-a197-b653bea80bed" class="">launch the kernel with <code>&lt;&lt;&lt;...&gt;&gt;&gt;</code></p></div></p><p id="64af2ab4-2507-4c45-8ec0-9f91aa2d27eb" class="">Kernel launch parameters:<div class="indented"><p id="1a6b417b-1ec3-41e1-b58d-11bbc24a9c0a" class=""><code>&lt;&lt;&lt;G, B&gt;&gt;&gt;</code> </p><p id="c00053d6-d16c-4068-b101-e45b966a3f82" class="">Type <code>dim3</code> is 3D type for threads and blocks</p><p id="1da725e6-22af-4879-b28c-dff8fd6f7019" class="">allows for indexing of elements as vector, matrix, or volume</p><p id="d3da2ef8-527c-4105-b09f-2aecae6977bd" class="">other type is <code>int</code> which specifies a 1D vector</p><p id="6d41b938-a6e5-4640-8fc1-c757f483fcb7" class="">G ⇒ G.x * G.y * G.z = # of blocks being launched</p><p id="baaea41d-3400-4e48-a9c2-ec7d006d1317" class="">B ⇒ B.x * B.y * B.z = # of threads per block</p><p id="3e56c0bf-5f27-4454-b6a5-5ca7dc593698" class="">total threads = (threads/block) * # of blocks</p><figure id="fedf869b-99f4-47ce-918c-aabf3cd7f54c" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2017.png"><img style="width:587.03125px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2017.png"/></a></figure></div></p><figure id="048ba4ba-07a2-4084-b06c-26f3b1e61569" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2018.png"><img style="width:1486px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2018.png"/></a></figure><ol type="1" id="54be8d6b-93ba-4060-be59-c5a1b7ce8210" class="numbered-list" start="1"><li>Kernel executes in a thread</li></ol><ol type="1" id="15115f7e-7a57-4e10-a698-bfc77873f155" class="numbered-list" start="2"><li>Threads grouped into Thread Blocks (aka Blocks)</li></ol><ol type="1" id="7f01c581-07da-44c1-b535-2c0d51846630" class="numbered-list" start="3"><li>Blocks grouped into a Grid</li></ol><ol type="1" id="cd91342c-ecd5-459d-b495-d30cffb50630" class="numbered-list" start="4"><li>Kernel executed as a Grid of Blocks of Threads</li></ol><p id="a83af369-26f3-4d42-a641-15cd3568ce92" class=""><code>cudaDeviceSynchronize();</code> ⇒ makes sure all the kernel for one problem are caught up so you can safely begin the next. Think of this as a barrier.</p><p id="0b358ed6-a863-47ca-9608-e60becf1e686" class="">Hardware mappings</p><p id="60baa05b-dbac-40ba-b1d1-22eb4c2f3ba3" class="">CUDA thread ⇒ single CUDA core (streaming processor)</p><p id="c62021be-20e4-4357-801f-e04e6155391e" class="">CUDA thread block ⇒ Streaming Multiprocessor</p><p id="993a59aa-88ac-43da-9927-dcedb967a3c4" class="">Kernel Grid ⇒ NVIDIA GPU</p><p id="7d393e28-3e8e-4521-9473-56b48c8dd034" class="">
</p><p id="7de5a5b4-aa47-45a2-b72b-dc14815b44f2" class=""><span style="border-bottom:0.05em solid">Run-time:</span><div class="indented"><p id="f56363b8-6582-4cb9-981b-a256a2ee3f23" class="">CPU invokes kernel grid by passing such instructions to the GPU</p><p id="3e238a2d-d89d-481f-81fb-f0c6ca7fee21" class="">Blocks on the grid distributed to Streaming Multiprocessors</p><p id="938b68a6-14a1-436d-ac4f-2e1b087c55ec" class="">Concurrent execution<div class="indented"><p id="c3d2a9a1-6d48-4124-a488-ecab08ee6e4f" class="">each SM runs multiple thread blocks through the lifetime of the program (one block at a time concurrently)</p><p id="44f252a0-df73-43f1-9986-e98f91629deb" class="">each CUDA core runs a thread from a Block</p></div></p></div></p><p id="9f10d8f9-e3e9-4072-b7f7-04beacbfde60" class="">
</p><p id="ab0d2e81-a8de-42dc-962c-3168bdb25ae3" class="">CUDA parallelism is scalable because their aren’t sequential block run-time dependencies. What I mean here is that you may not run Block 0 &amp; Block 1, then Block 2 &amp; 3… It may be Block 3 &amp; 0, then Block 6 &amp; 1. This means each of these mini “jobs” are solving a subset of the problem independent of the others. Like one piece of the puzzle. As long as all the pieces are assembled in the right place at the end, it works!</p><p id="c2c1a3a3-975d-4a82-ac15-c945d4ba470a" class="">
</p><p id="2a0bc06f-1e06-4f77-af9f-f3ce74afad08" class=""><code>h_A</code> refers to host (CPU) for variable name “A”</p><p id="1b226ad0-8d9a-463a-93aa-1bb6671d1906" class=""><code>d_A</code> refers to device (GPU) for variable name “A” </p><p id="337248bc-4e2d-418c-a15f-057f4a077d69" class=""><code>__global__</code> is visible globally, meaning the CPU or  <em>host</em> can call these global functions. these don’t typically return anything but just do really fast operations to a variable you pass in. for example, I could multiply matrix A and B together, but I need to pass in a matrix of the needed size as C and change the values in C to the outputs of A * B matmul. these are your cuda kernels </p><p id="68a08c09-de50-4cd3-9c37-28730afbcdbe" class=""><code>__device__</code> is a very cool function I haven’t dived into yet but this is the small job that only the GPU can call. GPT-4 really liked my example of having a raw attention score matrix living on the <code>__global__</code> gpu cuda kernel and it needs to apply a scalar mask. instead of also doing this in the cuda kernel, we can have a <code>__device__</code> function defined in another .cu file or just exist as a function in the same file that does this SIMD scalar masking on any matrix we give it. this is the cuda equivalent of calling a function in a library instead of writing the function in your <code><a href="http://main.py">main.py</a></code> file</p><p id="348fdaa4-f1ac-431b-ade2-78bedc798adc" class=""><code>__host__</code> is only going to run on CPU. same as running a regular c/c++ script on CPU without cuda.</p><ul id="67bc0ae1-748d-4d31-bbd5-3352b3e71bb4" class="bulleted-list"><li style="list-style-type:disc">Memory Model<ul id="c4d128d8-8627-443f-8e95-6efb39dfdf95" class="bulleted-list"><li style="list-style-type:circle">Registers &amp; Local Memory</li></ul><ul id="7b187ee8-b8b1-4be8-87a8-13cc2a785352" class="bulleted-list"><li style="list-style-type:circle">Shared Memory ⇒ allows threads within a block to communicate</li></ul><ul id="8799d5b6-2ebb-4dac-88e3-e05823fa4b71" class="bulleted-list"><li style="list-style-type:circle">Global Memory ⇒ Stores data copies to and from Host. Everything on device can access Global mem</li></ul><ul id="261e267c-450b-4f6b-a315-9ac586d314d2" class="bulleted-list"><li style="list-style-type:circle">Arrays too big to fit into the Register will spill into local memory. our goal is to make sure this doesn’t happen because we want to keep our program running as fast as possible</li></ul><figure id="8d9a743c-9174-43db-86b7-83248be2980f" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2019.png"><img style="width:2918px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2019.png"/></a></figure></li></ul><p id="6dc56b87-5e54-47df-bcff-f618e4bf1ab2" class="">THREADS:</p><ul id="63d16eed-ac18-4e12-a119-4a62a35fc02c" class="bulleted-list"><li style="list-style-type:disc">each thread has local memory (registers) and is private to the thread</li></ul><ul id="9c1893fe-fc4f-46ad-9eeb-4fb2676fdf93" class="bulleted-list"><li style="list-style-type:disc">if want to add <code>a = [1, 2, 3, ... N]</code> and <code>b = [2, 4, 6, ... N]</code> each thread would do a single add ⇒ <code>a[0] + b[0]</code> (thread 1); <code>a[1] + b[1]</code> (thread 2); etc...</li></ul><p id="f22506bd-ca7f-457f-9b1b-5eea4395d367" class="">WARPS:</p><figure id="82f58499-9f40-4dab-b8db-9971024db481" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2020.png"><img style="width:566px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2020.png"/></a></figure><ul id="ee6e5fdf-4b26-4817-ae67-1bcd389ec6a7" class="bulleted-list"><li style="list-style-type:disc"><a href="https://en.wikipedia.org/wiki/Warp_and_weft">https://en.wikipedia.org/wiki/Warp_and_weft</a></li></ul><ul id="76fcf1b5-232d-4825-bfb8-0af7face9870" class="bulleted-list"><li style="list-style-type:disc">The warp is the set of <a href="https://en.wikipedia.org/wiki/Yarn">yarns</a> or other things stretched in place on a <a href="https://en.wikipedia.org/wiki/Loom">loom</a> before the weft is introduced during the weaving process. It is regarded as the <em>longitudinal</em> set in a finished fabric with two or more sets of elements.</li></ul><ul id="c1202e39-bc25-4b5b-b1e9-22fe3ad73eb3" class="bulleted-list"><li style="list-style-type:disc">Each warp is inside of a block and parallelizes 32 threads</li></ul><ul id="63920e8d-d2c1-4ab4-8dbc-e0f30b9a6746" class="bulleted-list"><li style="list-style-type:disc">Instructions are issued to warps that then tell the threads what to do (not directly sent to threads)</li></ul><ul id="ef57cb76-685d-46c2-aa24-f604b15e5a69" class="bulleted-list"><li style="list-style-type:disc">There is no way of getting around using warps</li></ul><ul id="37a2de65-708d-4c1f-9629-8eb055007663" class="bulleted-list"><li style="list-style-type:disc">Warp scheduler makes the warps run</li></ul><p id="73f1f226-037f-4fab-8e57-dbf971352cbf" class="">BLOCKS:</p><ul id="67289e14-6a65-4d73-b056-37469b71e732" class="bulleted-list"><li style="list-style-type:disc">each block has shared memory (visible to all threads in thread block)</li></ul><ul id="590edc92-7d92-4b46-ab27-5792c6193c46" class="bulleted-list"><li style="list-style-type:disc">execute the same code on different data, shared memory space, more efficient memory reads and writes since coordination is better</li></ul><p id="4185bc13-ffb9-4702-bb8e-a0b6d7f09996" class="">GRIDS:</p><ul id="cc20426e-d5c0-43b1-88c2-36f913339ce7" class="bulleted-list"><li style="list-style-type:disc">during kernel execution, the threads within the blocks within the grid can access global memory (VRAM)</li></ul><ul id="aa8ccb75-be97-4472-905a-5ccd89ad3e2d" class="bulleted-list"><li style="list-style-type:disc">contain a bunch of blocks. best example is grids handle batch processing, where each block in the grid is a batch element</li></ul><p id="a1ea397e-e59c-43c8-bd66-0c1b0dcf88a0" class="">Memory Management:</p><ul id="76e95faa-c15c-4b8f-a716-319e9adc2a21" class="bulleted-list"><li style="list-style-type:disc"><code>cudaMalloc</code> memory allocation on VRAM only (also called global memory)</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="297a0525-4bd9-4236-bc8a-c89054452e7a" class="code"><code class="language-undefined">    float *d_a, *d_b, *d_c;

    cudaMalloc(&amp;d_a, N*N*sizeof(float));
    cudaMalloc(&amp;d_b, N*N*sizeof(float));
    cudaMalloc(&amp;d_c, N*N*sizeof(float));</code></pre><ul id="95337a7d-a3c0-4764-9cca-5ebf7460ed63" class="bulleted-list"><li style="list-style-type:disc"><code>cudaMemcpy</code> can copy from device to host, host to device, or device to device (edge cases)<ul id="888d4d90-20d7-49dd-badd-e46eb458c12e" class="bulleted-list"><li style="list-style-type:circle">host to device ⇒ CPU to GPU</li></ul><ul id="3d6252ac-7177-4e3e-b105-80c0a7a77008" class="bulleted-list"><li style="list-style-type:circle">device to host ⇒ GPU to CPU</li></ul><ul id="a482a7e2-9a4b-4587-a034-12a2d0dd1a54" class="bulleted-list"><li style="list-style-type:circle">device to device ⇒ GPU location to different GPU location</li></ul><ul id="2eca3207-965a-4f20-8c6b-fe018d2e0e2f" class="bulleted-list"><li style="list-style-type:circle"><code><strong>cudaMemcpyHostToDevice</strong></code>, <code><strong>cudaMemcpyDeviceToHost</strong></code>, or <code><strong>cudaMemcpyDeviceToDevice</strong></code></li></ul></li></ul><ul id="201661f0-cd75-44ba-b219-d497a4272e85" class="bulleted-list"><li style="list-style-type:disc"><code>cudaFree</code> will free memory on the device</li></ul><p id="e26d3b56-ec38-4606-91a6-778bf4791510" class="">SIMT Architecture</p><ul id="e149cd7e-8415-42fc-bc52-f842bcd6d0c1" class="bulleted-list"><li style="list-style-type:disc">similar to CPU SIMD (single instruction multiple data), we have single instruction multiple thread on GPU. </li></ul><ul id="1df9438c-4ef3-4091-ad96-cc4f95fdff69" class="bulleted-list"><li style="list-style-type:disc">Instead of running the for loop sequentially, each thread can run a single iteration of the for loop so that it appears to only take the time of one iteration. it can grow linearly if you add more and more iterations as you would expect (not enough cores to parallel process all the independent iterations of the for loop)</li></ul><ul id="a8990481-6f15-4b17-b8fe-b49b6a845137" class="bulleted-list"><li style="list-style-type:disc">Simpler than CPU<ul id="acb8c320-866e-4bf5-9e25-7d1c956b414e" class="bulleted-list"><li style="list-style-type:circle">in-order instruction issue</li></ul><ul id="aafdbbd0-1991-4533-9ba7-d5c8c9bb319e" class="bulleted-list"><li style="list-style-type:circle">no branch prediction</li></ul><ul id="0245879f-74b6-4241-9a6d-73b54e38d52b" class="bulleted-list"><li style="list-style-type:circle">significantly less control than CPU architecture gives us more room for more CORES</li></ul></li></ul><ul id="3eb5246f-a17a-405b-b3f8-237798a028a2" class="bulleted-list"><li style="list-style-type:disc">Thread Syncronization w/ Explicit Barrier ⇒ <code>__syncthreads();</code></li></ul><figure id="0f2580be-7380-4c3a-a8a4-aae0a9e8eb59" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2021.png"><img style="width:532px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2021.png"/></a></figure><ul id="369e1062-4f4a-4088-85f1-2f88f3920164" class="bulleted-list"><li style="list-style-type:disc">example is when you are shifting all elements in array one index to the left</li></ul><figure id="63849482-d966-4e33-a649-1949b4113635" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2022.png"><img style="width:2864px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2022.png"/></a></figure><figure id="96c9c401-5e48-4541-b263-1551d63e8d9e" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2023.png"><img style="width:506px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2023.png"/></a></figure><p id="f53d014b-76c0-4460-ba1d-688a97b451c6" class="">its generally a good idea to write code for a kernel first on CPU (easy to write), then on GPU to ensure your logic lines up on the level of blocks and threads. you can set some input x, feed it through the CPU function and GPU kernel, check if outputs are the same. this tells you if your GPU code is working as expected</p><p id="00f4e2f2-3e04-4ebf-9acc-464c4bf198f6" class="">error checking ⇒ look into cudacheck and error macros</p></details></li></ul><ul id="56d36cee-6fd6-4a32-bf35-3c2f2b75bba5" class="toggle"><li><details open=""><summary><em><strong>Profiling</strong></em></summary><ul id="5c3f0309-0a49-4a34-922f-0d7f28325a3f" class="bulleted-list"><li style="list-style-type:disc"><code>nvtop</code>, <code>nvitop</code>, <code>nvidia-smi</code></li></ul><ul id="91ab8ace-32b7-4b96-a054-f64340fe5464" class="bulleted-list"><li style="list-style-type:disc">Nsight systems &amp; compute ⇒ <code>nsys profile --stats=true ./main</code> <ul id="2da4f798-4e3e-4eba-bee3-5826182b36f7" class="bulleted-list"><li style="list-style-type:circle">Unless you have a specific profiling goal, the suggested profiling strategy is starting with Nsight Systems to determine system bottlenecks and identifying kernels that affect performance the most. On a second step, you can use Nsight Compute to profile the identified kernels and find ways to optimize them.</li></ul><ul id="5f3837cf-9bb2-4513-9a16-15f3c4a2c613" class="bulleted-list"><li style="list-style-type:circle"><a href="https://stackoverflow.com/questions/76291956/nsys-cli-profiling-guidance">https://stackoverflow.com/questions/76291956/nsys-cli-profiling-guidance</a></li></ul><ul id="ac6e8b5b-487a-4737-a6aa-b131f98a0f50" class="bulleted-list"><li style="list-style-type:circle">if you already have the <code>.nsys-rep</code> file, run <code>nsys stats file.nsys-rep</code> for a more quantitative profile. for <code>.sqlite</code> run <code>nsys analyze file.sqlite</code> to give a more qualitative profile</li></ul><ul id="c6cd7d6f-93ae-444b-836f-191bdb589fce" class="bulleted-list"><li style="list-style-type:circle">to see a detailed GUI of this, I can run <code>nsight-sys</code> ⇒ file ⇒ open ⇒ rep file</li></ul><ul id="1fd3a22c-1087-47fd-9dbf-b14e4cbcd8be" class="bulleted-list"><li style="list-style-type:circle"><code>nsys</code> nsight systems is higher level; <code>ncu</code> nsight compute is lower level</li></ul><ul id="d2e087a3-9406-409c-b7b2-6cb32f4fa832" class="bulleted-list"><li style="list-style-type:circle">generate profiling files for python script <code>nsys profile --stats=true -o mlp python </code><code><a href="http://mlp.py/">mlp.py</a></code></li></ul><ul id="8b9ea8db-9c7a-4dd8-8b2b-44e3b2150d33" class="bulleted-list"><li style="list-style-type:circle">to profile w/ nsight systems GUI, find the kernels you need to optimize (ex: <code>ampere_sgemm</code>), open in event view, zoom to selected on timeline, analyze kernel w/ ncu by right clicking on timeline</li></ul><ul id="15ef43ee-9cfa-47c8-8fb7-06576ebe8466" class="bulleted-list"><li style="list-style-type:circle">ncu may deny permissions ⇒ <code>code /etc/modprobe.d/nvidia.conf</code> and force change nvidia.conf by adding the line <code>options nvidia NVreg_RestrictProfilingToAdminUsers=0</code> then restarting your machine. This was the only issue I ran into so you may be on your own through some issues I don’t encounter in this course. check github, stackoverflow, nvidia developer forums, nvidia docs, pytorch docs if your issue is related to CUDA OR triton in pytorch, chatGPT or other LLMs to help navigate the space more easily (information won’t be as hard to process since its neatly organized)<ul id="6df9c242-92fb-41c3-a723-471c2a51c3b4" class="bulleted-list"><li style="list-style-type:square">src ⇒ <a href="https://developer.nvidia.com/nvidia-development-tools-solutions-err_nvgpuctrperm-permission-issue-performance-counters">https://developer.nvidia.com/nvidia-development-tools-solutions-err_nvgpuctrperm-permission-issue-performance-counters</a></li></ul></li></ul><ul id="ef716572-fd4e-4d45-ba34-906bdfd8c892" class="bulleted-list"><li style="list-style-type:circle">the following code gives me the below results w/ <code>ncu</code></li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="fb3da927-1b84-48d1-a6a7-45c8c67f29db" class="code"><code class="language-C"># just a sample mlp model for profiling -&gt; y = sin(x)

import torch
import torch.nn as nn
import torch.nn.functional as F

class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(2048, 2048)
        self.fc2 = nn.Linear(2048, 2048)
        self.fc3 = nn.Linear(2048, 2048)
        self.fc4 = nn.Linear(2048, 2048)
        self.fc5 = nn.Linear(2048, 2048)
        self.fc6 = nn.Linear(2048, 2048)
        self.fc7 = nn.Linear(2048, 2048)
        self.fc8 = nn.Linear(2048, 2048)
        self.fc9 = nn.Linear(2048, 2048)
        self.fc10 = nn.Linear(2048, 2048)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = F.relu(self.fc4(x))
        x = F.relu(self.fc5(x))
        x = F.relu(self.fc6(x))
        x = F.relu(self.fc7(x))
        x = F.relu(self.fc8(x))
        x = F.relu(self.fc9(x))
        x = F.relu(self.fc10(x))
        return x

model = MLP()
model.to(&quot;cuda&quot;)
x = torch.randn((2048, 2048), device=&quot;cuda&quot;)
out = model(x)


# Warmup
for _ in range(10):
    out = model(x)

# Custom kernel run
import time
start1 = time.time()
out = model(x)
end1 = time.time()

</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="c1588473-28eb-4f43-a07b-e8c52a107a6a" class="code"><code class="language-C">==PROF== Connected to process 30412 (/home/elliot/.pyenv/versions/3.11.7/bin/python3.11)
==PROF== Profiling &quot;ampere_sgemm_128x64_tn&quot;: 0%....50%....100% - 8 passes
==PROF== Disconnected from process 30412
[30412] python3.11@127.0.0.1
  ampere_sgemm_128x64_tn (16, 32, 3)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.79
    SM Frequency            cycle/nsecond         1.48
    Elapsed Cycles                  cycle    2,471,848
    Memory Throughput                   %        61.88
    DRAM Throughput                     %        29.87
    Duration                      msecond         1.66
    L1/TEX Cache Throughput             %        62.99
    L2 Cache Throughput                 %        56.64
    SM Active Cycles                cycle 2,422,188.39
    Compute (SM) Throughput             %        72.07
    ----------------------- ------------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  1,536
    Registers Per Thread             register/thread             122
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           12.80
    # SMs                                         SM              46
    Threads                                   thread         196,608
    Uses Green Context                                             0
    Waves Per SM                                                8.35
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            4
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        32.30
    Achieved Active Warps Per SM           warp        15.51
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel&#x27;s theoretical occupancy (33.3%) is limited by the number of required      
          registers. This kernel&#x27;s theoretical occupancy (33.3%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    3,368,914
    Total DRAM Elapsed Cycles        cycle   90,235,904
    Average L1 Active Cycles         cycle 2,422,188.39
    Total L1 Elapsed Cycles          cycle  113,416,280
    Average L2 Active Cycles         cycle 2,354,779.41
    Total L2 Elapsed Cycles          cycle   75,495,776
    Average SM Active Cycles         cycle 2,422,188.39
    Total SM Elapsed Cycles          cycle  113,416,280
    Average SMSP Active Cycles       cycle 2,421,949.55
    Total SMSP Elapsed Cycles        cycle  453,665,120
    -------------------------- ----------- ------------

</code></pre><ul id="31fdd6ce-e182-47fe-8050-dbae1cb1b555" class="bulleted-list"><li style="list-style-type:circle">what are memory access patterns?</li></ul></li></ul><ul id="675ac2c8-3ed4-45f7-93b1-41bb67fd2303" class="bulleted-list"><li style="list-style-type:disc">memory debugging ⇒ <code>compute-sanitizer ./main</code></li></ul><ul id="d238fc04-a39b-4d6b-aed0-cd69726e5690" class="bulleted-list"><li style="list-style-type:disc">kernel performance UI ⇒ <code>ncu-ui</code> (might have to install <code>sudo apt-get install libxcb-cursor0</code>)</li></ul><ul id="1fe85290-16e5-49e9-865d-1df85c8d182d" class="bulleted-list"><li style="list-style-type:disc">when profiling the following 3 variants with a 32 (2^25) million element vector addition<ul id="e3483180-27d9-45b8-84f5-abc22a9a587e" class="bulleted-list"><li style="list-style-type:circle">basic without blocks OR threads</li></ul><figure id="afa54c1c-9576-4585-bf12-aa7a7e2de6aa" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2024.png"><img style="width:722px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2024.png"/></a></figure><ul id="52ad3da5-5834-4a91-aac2-c8d347527dd4" class="bulleted-list"><li style="list-style-type:circle">w/ threads</li></ul><figure id="f6e825cf-7237-42dc-bc34-960ac31ab147" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2025.png"><img style="width:624px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2025.png"/></a></figure><ul id="b99ad86b-be8f-45f7-838e-cef83a60459f" class="bulleted-list"><li style="list-style-type:circle">w/ threads + blocks</li></ul><figure id="b8fd4f93-9e70-4a3b-84e5-b113607b4f39" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2026.png"><img style="width:924px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2026.png"/></a></figure><ul id="7ca1e2ab-3bec-4da1-8481-ca9683a690dd" class="bulleted-list"><li style="list-style-type:circle">notice how this scales slower relative to older GPUs. it might be the memory available or that I’m using <code>nsys</code> instead of the tutorial’s <code>nvprof</code> for older hardware</li></ul><ul id="c678f242-ebcd-4cdd-89bb-a5a5bffc3481" class="bulleted-list"><li style="list-style-type:circle">originally from: <a href="https://developer.nvidia.com/blog/even-easier-introduction-cuda/">https://developer.nvidia.com/blog/even-easier-introduction-cuda/</a></li></ul><ul id="fa63b5ab-8d39-410b-bc7d-4614d5cdfe48" class="bulleted-list"><li style="list-style-type:circle">when using FFT for a very large window or batch size (ex: batchsize = 1, kernelsize = 128, image_size_size = 2048), you get a 208x speedup w/ manual cuda kernels. under the hood, libs like pytorch and cuDNN will select the correct (manually VS FFT) convolve operation logic based on the function arguments (combinations of function arguments relative to time ⇒ theoretical VS practical complexity)</li></ul></li></ul></details></li></ul><ul id="75743d03-2e23-42bf-b9c1-00e5c099ed82" class="toggle"><li><details open=""><summary>Triton kernel example vs cuda kernel (vecAdd)</summary><p id="498cbc38-c1df-4926-b820-a1d940574ae4" class="">
</p></details></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.5em;line-height:1.3;margin:0">CUDA API / Libs</summary><div class="indented"><ul id="0bd4c470-5410-4722-88bc-b4a8053d1642" class="toggle"><li><details open=""><summary>Overview</summary><figure id="11d1200c-1018-4112-88f7-8a5e01d8773a" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2027.png"><img style="width:1061px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2027.png"/></a></figure><p id="ab545bfc-0b1c-4166-96dd-fb77108d70ec" class="">the term “API” can be confusing at first. all this mean is we have a library where we can’t see the internals. there is documentation on the function calls within the API, but its a precompiled binary that doesn’t expose source code. the code is highly optimized but you can’t see it. (keep this here as it universally applies to all the libs/APIs listed below)</p><p id="78c54102-a04b-412c-b268-8c47e9e825b7" class="">
</p><p id="d8569958-da8c-4057-b396-536aa7c3783a" class="">Opaque Struct Types (CUDA API):</p><p id="52609363-3e5d-4679-8271-4aa16253a48d" class="">you cannot see or touch the internals of the type, just external like names, function args, etc. <code>.so</code> (shared object) file referenced as an opaque binary to just run the compiled functions at high throughput. If you search up cuFFT, cuDNN, or any other CUDA extension, you will notice it comes as an API, the inability to see through to the assembly/C/C++ source code refers to usage of the word “opaque”. the struct types are just a general type in C that allows NVIDIA to build the ecosystem properly. </p><p id="3a301982-f8a5-4909-8e7b-c8338f6f76c8" class="">
</p><p id="5dfa45ac-fd2a-43f5-a20f-ac3879717aa9" class="">if you’re trying to just figure out how to get the fastest possible inference to work on your cluster, you will need to understand the details under the hood. To navigate the CUDA API, I’d recommend using the following tricks:</p><ol type="1" id="08795bed-ed52-4f5f-86cd-3b460f90947f" class="numbered-list" start="1"><li><a href="http://perplexity.ai">perplexity.ai</a> (most up to date information and will fetch data in real time)</li></ol><ol type="1" id="06be9b07-24d7-4195-8033-39c63861e437" class="numbered-list" start="2"><li>google search (arguably worse than perplexity but its alright to take the classic approach to figuring things out)</li></ol><ol type="1" id="c0485fb0-4248-47c7-b78f-3eab8acbe6cf" class="numbered-list" start="3"><li>chatGPT for general knowledge that’s less likely to be past its training cutoff</li></ol><ol type="1" id="91f792cd-9c7b-4670-aec7-71f80ef43f28" class="numbered-list" start="4"><li>keyword search in nvidia docs</li></ol></details></li></ul><ul id="6f69bcaa-860f-4a12-b691-c2825663be82" class="toggle"><li><details open=""><summary>cuRAND (don’t present this in lecture but good to know regardless)</summary><h3 id="d17330db-96a9-4923-ac02-1bbe796271d1" class="">Intro (<a href="https://docs.nvidia.com/cuda/curand/index.html">https://docs.nvidia.com/cuda/curand/index.html</a>):</h3><p id="1872a57d-31a5-48d8-a15b-95f2ad9b12b9" class="">The cuRAND library provides facilities that focus on the simple and efficient generation of high-quality pseudorandom and quasirandom numbers. A pseudorandom sequence of numbers satisfies most of the statistical properties of a truly random sequence but is generated by a deterministic algorithm. A quasirandom sequence of 𝑛 -dimensional points is generated by a deterministic algorithm designed to fill an 𝑛 -dimensional space evenly.</p><p id="98a26063-3773-4be7-8957-6b54cadb51ab" class="">cuRAND consists of two pieces: a library on the host (CPU) side and a device (GPU) header file. The host-side library is treated like any other CPU library: users include the header file, /include/curand.h, to get function declarations and then link against the library. Random numbers can be generated on the device or on the host CPU. For device generation, calls to the library happen on the host, but the actual work of random number generation occurs on the device. The resulting random numbers are stored in global memory on the device. Users can then call their own kernels to use the random numbers, or they can copy the random numbers back to the host for further processing. For host CPU generation, all of the work is done on the host, and the random numbers are stored in host memory.</p><p id="618e1966-4f9f-4c3b-a604-9bc6ee409e86" class="">The second piece of cuRAND is the device header file, /include/curand_kernel.h. This file defines device functions for setting up random number generator states and generating sequences of random numbers. User code may include this header file, and user-written kernels may then call the device functions defined in the header file. This allows random numbers to be generated and immediately consumed by user kernels without requiring the random numbers to be written to and then read from global memory.</p><p id="3ebd8d5d-e537-4362-9052-fea5a2a7ba29" class="">SUMMARY: you have host (cpu) and device (gpu) random generators. there are a bunch of random gen algos which generate nearly completely random numbers given a “seed” for reproducibility. this means that if I want to generate 1 million random numbers with seed 314, </p><p id="1f2f75e2-b8ce-4623-9d2a-54c58a2dd0b5" class="">I can use 314 again whenever I want to get those same random numbers. you just need to be using the same precision (default int64 I think), algorithms, tensor size (maybe this is just a different interpretation of <em>offset</em> &amp; <em>order</em>), and the seed itself. </p><p id="e499e6fb-2ae3-4ecb-939d-223250eac62c" class="">for the purpose of this course, we would only use random numbers for weight inits, which is negligible in the whole training process (we care about faster forward and backward passes in the duration of the training/inference run versus the first few seconds of initialization)</p><p id="06e11ada-189a-4641-9d5a-610fb08395ab" class=""><span style="border-bottom:0.05em solid">Seed:</span></p><p id="efc547b8-023e-48d9-802e-6b755c77635b" class="">The seed parameter is a 64-bit integer that initializes the starting state of a pseudorandom number generator. The same seed always produces the same sequence of results.</p><p id="32e5a890-dce4-4831-911f-7d6bf410f736" class=""><span style="border-bottom:0.05em solid">Offset:</span></p><p id="f909921d-09ea-4dab-980c-18a08a22169a" class="">The offset parameter is used to skip ahead in the sequence. If offset = 100, the first random number generated will be the 100th in the sequence. This allows multiple runs of the same program to continue generating results from the same sequence without overlap. Note that the skip ahead function is not available for the CURAND_RNG_PSEUDO_MTGP32 and CURAND_RNG_PSEUDO_MT19937 generators.</p><p id="ffc17c05-9213-488d-80e9-04b1d2f5294d" class=""><span style="border-bottom:0.05em solid">Order:</span></p><p id="06538693-1a39-4a8b-9feb-6e3409a4ccbe" class="">lots of content so read here ⇒ <a href="https://docs.nvidia.com/cuda/curand/host-api-overview.html#order">https://docs.nvidia.com/cuda/curand/host-api-overview.html#order</a></p><p id="ad102f2e-c850-408e-9ca6-91efbdfd2153" class="">
</p><ul id="7ca1dab5-308d-435d-ae6f-26698dfbfc64" class="bulleted-list"><li style="list-style-type:disc"><strong>CPU</strong>: On the CPU, PyTorch primarily uses the <strong>MT19937</strong> variant of the Mersenne Twister (referenced by the <code>std::mt19937</code> class in C++) as its random number generator.</li></ul><ul id="e6679b79-e34b-4232-bbf0-55f2a194786f" class="bulleted-list"><li style="list-style-type:disc"><strong>GPU</strong>: On GPU, for CUDA operations, PyTorch employs the <strong>Philox</strong> RNG. This is part of the CUDNN library and is optimized for parallel execution on GPU architectures.</li></ul><p id="62f5636f-e4b5-4d48-8619-fcec8502f90e" class="">how does the random generator work on the level of physics and raw computation?</p><ul id="53e68141-ea06-484c-85c5-0e31af1331ea" class="bulleted-list"><li style="list-style-type:disc">we need two things: source of entropy (messiness, disorder, dynamic) AND an algorithm to further mix up this messiness<ul id="2ce24a31-d9eb-4bd0-abaa-58e1b2be49cc" class="bulleted-list"><li style="list-style-type:circle">we can achieve a source of entropy from time itself, if we feed in the time to the last second, we can mash up the emerging numbers and achieve something pseudorandom. a “more” random set of numbers would come from algorithms with more “mixing” steps and a better source of entropy. there are different algorithms researchers have invented. other sources of entropy include physically properties about/around the hardware itself like: temperature, phase noise, clock signals, and more. </li></ul></li></ul></details></li></ul><ul id="7af3c3a8-5e3f-453b-82a6-b3a7993d3c73" class="toggle"><li><details open=""><summary>cuBLAS ( &amp; variants )</summary><p id="eb2b3051-4d27-4d46-b185-3db428a62bf4" class="">out of the box performance better than the next topic <em>CUTLASS</em> ⇒ cuBLAS Time: 0.202861 ms; CUTLASS Time: 0.227451 ms</p><p id="77964f50-99d3-4508-88bc-a2cc2a1f36b0" class="">The plan is to cover cuBLAS functionality, understand that most of the operations are on the level of tensors, and stick with cuDNN/ CUTLASS</p><h3 id="4760e088-88e1-4ad7-9d01-6a5407ef5666" class="">Intro</h3><p id="3bf4be0b-9ed2-4258-ac02-b03b61797145" class="">NVIDIA CUDA Basic Linear Algebra Subprograms is <strong>a GPU-accelerated library for accelerating AI and HPC (high performance compute) applications</strong>. It includes several API extensions for providing drop-in industry standard BLAS APIs and GEMM (general matrix multiplication) APIs with support for fusions that are highly optimized for NVIDIA GPUs.</p><p id="73159b7b-4725-4e44-9b80-42728714b3b7" class="">
</p><h2 id="8bef0e42-9325-4f6c-8b26-4aa3dd930670" class="">cuBLAS</h2><p id="1db68330-567b-427d-8b61-ac0be60162a3" class=""><strong>cuBLAS</strong> is the primary library for performing GPU-accelerated linear algebra operations, including matrix and vector operations like matrix-matrix multiplication (GEMM), matrix-vector multiplication (GEMV), and vector operations (AXPY). It is designed to be a direct replacement for the CPU-based BLAS libraries but runs on NVIDIA GPUs.</p><p id="35299717-f878-47b9-b45a-b5736781488f" class="">
</p><h2 id="0fed1889-276c-4c5e-8708-b3f13c98548a" class="">cuBLASLt (<strong>cuda BLAS Lightweight)</strong></h2><p id="613a57fa-26dd-4634-afb0-a98c98b743a7" class=""><strong>cuBLASLt (cuda BLAS Lightweight)</strong> is an extension of the cuBLAS library that provides a more flexible API, primarily aimed at improving performance for specific workloads like deep learning models. It offers:</p><ol type="1" id="ac8d3830-e89b-45ca-b010-e6a34ac6b48c" class="numbered-list" start="1"><li><strong>Customizability:</strong> More control over the computation through a flexible API, allowing users to customize aspects like algorithm selection, workspace size, and tuning parameters.</li></ol><ol type="1" id="6e9a1308-24d5-4905-8fab-36e90064c4db" class="numbered-list" start="2"><li><strong>High Performance:</strong> Optimized for performance-critical paths and tailored for deep learning and HPC applications.</li></ol><ol type="1" id="3fdd105d-613e-4f55-bf5f-c31e4e5bd16c" class="numbered-list" start="3"><li><strong>Reduced Overhead:</strong> Streamlined API calls to reduce the overhead associated with setting up operations, making it suitable for scenarios where frequent, repetitive computations are required.</li></ol><h2 id="1933ed3e-cf0f-460d-ad9f-0ba63769f18a" class="">cuBLASXt</h2><p id="b5fe2058-c79a-41e5-acc6-bf4de97698cc" class=""><strong>cuBLASXt</strong> is an extension to cuBLAS that enables multi-GPU support. Key features include:</p><ol type="1" id="0a503e2c-4006-4867-9f73-4137fb497992" class="numbered-list" start="1"><li><strong>Multiple GPUs:</strong> Ability to run BLAS operations across multiple GPUs, allowing for GPU scaling and potentially significant performance improvements on large datasets.</li></ol><ol type="1" id="76c1c568-a93f-43b7-9e4e-709b1279165c" class="numbered-list" start="2"><li><strong>Thread Safety:</strong> Designed to be thread-safe, enabling concurrent execution of multiple BLAS operations on different GPUs.</li></ol><ol type="1" id="018a957d-0b32-46f5-93b5-dec995aa6348" class="numbered-list" start="3"><li><strong>Cost-effective Scalability:</strong> Ideal for large-scale computations that can benefit from distributing workloads across multiple GPUs.</li></ol><h2 id="14a2d104-6314-4feb-bfa8-4aab5fe197cb" class="">cuBLASDx</h2><p id="78b64b59-5a8b-425b-84f7-847f040db32e" class="">Highlight that we <strong><span style="border-bottom:0.05em solid">ARE NOT</span></strong> using this in the course</p><p id="1a5b1bb2-aaa5-4cc2-9c63-9a742968999d" class="">The cuBLASDx library (preview) is a device side API extension for performing BLAS calculations inside CUDA kernels. By fusing numerical operations you can decrease latency and further improve performance of your applications.</p><ul id="a0411896-5670-43e9-9327-a06e2d59c3ea" class="bulleted-list"><li style="list-style-type:disc">You can access cuBLASDx documentation <a href="https://docs.nvidia.com/cuda/cublasdx">here</a>.</li></ul><ul id="b1023432-7b8d-4581-9490-279e9ff7e191" class="bulleted-list"><li style="list-style-type:disc">cuBLASDx is not a part of the CUDA Toolkit. You can download cuBLASDx separately from <a href="https://developer.nvidia.com/cublasdx-downloads">here</a>.</li></ul><p id="9162e026-7aba-43ea-8016-51d4b88a2041" class="">
</p><p id="ed4f5de3-8174-4bf5-9dc4-9342c4cec78f" class="">
</p></details></li></ul><ul id="623f8a18-0168-4220-93d0-5d6edf7d1b05" class="toggle"><li><details open=""><summary>CUTLASS</summary><ul id="ff9a5029-0822-45bf-bfd6-dc73b9afe4ba" class="bulleted-list"><li style="list-style-type:disc">CUDA Templates for Linear Algebra Subroutines and Solvers ⇒ <a href="https://github.com/NVIDIA/cutlass/">https://github.com/NVIDIA/cutlass/</a> &amp; <a href="https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/">https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/</a></li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="564a7664-e0f9-46b2-9cb7-cee0fee0dbd9" class="code"><code class="language-C">include/                     # client applications should target this directory in their build&#x27;s include paths

  cutlass/                   # CUDA Templates for Linear Algebra Subroutines and Solvers - headers only

    arch/                    # direct exposure of architecture features (including instruction-level GEMMs)

    conv/                    # code specialized for convolution

    epilogue/                # code specialized for the epilogue of gemm/convolution

    gemm/                    # code specialized for general matrix product computations

    layout/                  # layout definitions for matrices, tensors, and other mathematical objects in memory

    platform/                # CUDA-capable Standard Library components

    reduction/               # bandwidth-limited reduction kernels that do not fit the &quot;gemm&quot; model

    thread/                  # simt code that can be performed within a CUDA thread
    
    transform/               # code specialized for layout, type, and domain transformations

    *                        # core vocabulary types, containers, and basic numeric operations

  cute/                      # CuTe Layout, layout algebra, MMA/Copy atoms, tiled MMA/Copy

    algorithm/               # Definitions of core operations such as copy, gemm, and operations on cute::tuples

    arch/                    # Bare bones PTX wrapper structs for copy and math instructions

    atom/                    # Meta-information either link to or built from arch/ operators

      mma_atom.hpp           # cute::Mma_Atom and cute::TiledMma

      copy_atom.hpp          # cute::Copy_Atom and cute::TiledCopy

      *sm*.hpp               # Arch specific meta-information for copy and math operations

    *                        # Core library types such as Shape, Stride, Layout, Tensor, and associated operations
</code></pre><ul id="2d4802de-32be-4b8e-a162-966e7cede3d9" class="bulleted-list"><li style="list-style-type:disc">Most commonly used for Matrix multiplication since its the heart of the transformer architecture so we care about <code>cutlass/gemm</code> </li></ul><ul id="b094be28-ec2d-4a4f-a221-067f0d19e96e" class="bulleted-list"><li style="list-style-type:disc">you likely won’t ever use cutlass just because of the overarching complexity. cutlass is designed for kernel engineers to fine-tune and optimize around specific hardware architectures whilst requiring deep GPU kernel knowledge.</li></ul><ul id="dcc13ecd-39bc-4dba-b179-c1484e738fa6" class="bulleted-list"><li style="list-style-type:disc">in the comparison script below, I compare the time taken for matrix multiplication on cuBLAS vs CUTLASS. we use 1024x1024x1024 matrices, warmup cuBLAS with 10 matmuls then record time, then do the same for CUTLASS. </li></ul><ul id="73132740-c7a7-4db0-8dd9-71eb9cf432bd" class="bulleted-list"><li style="list-style-type:disc">This is performance difference (not super massive but we will happily take the ~10% boost w/ cuBLAS)<p id="d7a6cd6a-edf7-492d-8518-4204ee4bdbed" class="">cuBLAS Time: 0.202861 ms<br/>CUTLASS Time: 0.227451 ms<br/></p></li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="e770e61d-6448-4f82-a6b0-bf4769ba0c48" class="code"><code class="language-C">#include &lt;iostream&gt;
#include &lt;vector&gt;
#include &lt;random&gt;
#include &lt;chrono&gt;
#include &lt;cublas_v2.h&gt;
#include &lt;cutlass/gemm/device/gemm.h&gt;
#include &lt;cutlass/util/host_tensor.h&gt;
#include &lt;cutlass/util/reference/device/gemm.h&gt;
#include &lt;cutlass/util/reference/host/gemm.h&gt;
#include &lt;cutlass/util/tensor_view_io.h&gt;

#define CHECK_CUDA(call) \
    if((call) != cudaSuccess) { \
        std::cerr &lt;&lt; &quot;CUDA error at: &quot; &lt;&lt; __FILE__ &lt;&lt; &quot;:&quot; &lt;&lt; __LINE__ &lt;&lt; &quot; : &quot; &lt;&lt; cudaGetErrorString(call) &lt;&lt; std::endl; \
        exit(1); \
    }

#define CHECK_CUBLAS(call) \
    if((call) != CUBLAS_STATUS_SUCCESS) { \
        std::cerr &lt;&lt; &quot;cuBLAS error at: &quot; &lt;&lt; __FILE__ &lt;&lt; &quot;:&quot; &lt;&lt; __LINE__ &lt;&lt; std::endl; \
        exit(1); \
    }

void verify_results(const std::vector&lt;float&gt;&amp; A, const std::vector&lt;float&gt;&amp; B, int rows, int cols) {
    float epsilon = 1e-2; // Adjust if necessary
    for (int i = 0; i &lt; rows * cols; ++i) {
        if (std::abs(A[i] - B[i]) &gt; epsilon) {
            std::cerr &lt;&lt; &quot;Mismatch at index &quot; &lt;&lt; i &lt;&lt; &quot;: A[i] = &quot; &lt;&lt; A[i] &lt;&lt; &quot;, B[i] = &quot; &lt;&lt; B[i] &lt;&lt; std::endl;
            exit(1);
        }
    }
    std::cout &lt;&lt; &quot;Outputs match within tolerance.&quot; &lt;&lt; std::endl;
}

int main() {
    int m = 1024, n = 1024, k = 1024;
    size_t bytes_A = m * k * sizeof(float);
    size_t bytes_B = k * n * sizeof(float);
    size_t bytes_C = m * n * sizeof(float);

    // Allocate host memory
    std::vector&lt;float&gt; h_A(m * k);
    std::vector&lt;float&gt; h_B(k * n);
    std::vector&lt;float&gt; h_C_cublas(m * n, 0.0f);
    std::vector&lt;float&gt; h_C_cutlass(m * n, 0.0f);

    // Initialize matrices with random values
    std::mt19937 rng(std::random_device{}());
    std::uniform_real_distribution&lt;float&gt; dist(0.0f, 1.0f);
    for (int i = 0; i &lt; m * k; ++i) h_A[i] = dist(rng);
    for (int i = 0; i &lt; k * n; ++i) h_B[i] = dist(rng);

    float *d_A, *d_B, *d_C;
    CHECK_CUDA(cudaMalloc(&amp;d_A, bytes_A));
    CHECK_CUDA(cudaMalloc(&amp;d_B, bytes_B));
    CHECK_CUDA(cudaMalloc(&amp;d_C, bytes_C));

    CHECK_CUDA(cudaMemcpy(d_A, h_A.data(), bytes_A, cudaMemcpyHostToDevice));
    CHECK_CUDA(cudaMemcpy(d_B, h_B.data(), bytes_B, cudaMemcpyHostToDevice));

    // cuBLAS
    cublasHandle_t handle;
    CHECK_CUBLAS(cublasCreate(&amp;handle));
    const float alpha = 1.0f;
    const float beta = 0.0f;

    // Warmup cuBLAS
    for (int i = 0; i &lt; 10; ++i) {
        CHECK_CUBLAS(cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, m, n, k, &amp;alpha, d_A, m, d_B, k, &amp;beta, d_C, m));
    }
    CHECK_CUDA(cudaDeviceSynchronize());

    auto start = std::chrono::high_resolution_clock::now();
    CHECK_CUBLAS(cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, m, n, k, &amp;alpha, d_A, m, d_B, k, &amp;beta, d_C, m));
    CHECK_CUDA(cudaDeviceSynchronize());
    auto end = std::chrono::high_resolution_clock::now();
    std::chrono::duration&lt;float, std::milli&gt; cublas_time = end - start;
    std::cout &lt;&lt; &quot;cuBLAS Time: &quot; &lt;&lt; cublas_time.count() &lt;&lt; &quot; ms&quot; &lt;&lt; std::endl;

    CHECK_CUDA(cudaMemcpy(h_C_cublas.data(), d_C, bytes_C, cudaMemcpyDeviceToHost));

    // CUTLASS
    using ColumnMajor = cutlass::layout::ColumnMajor;
    using Gemm = cutlass::gemm::device::Gemm&lt;float, ColumnMajor, float, ColumnMajor, float, ColumnMajor&gt;;

    Gemm gemm_op;
    Gemm::Arguments args({m, n, k}, {d_A, m}, {d_B, k}, {d_C, m}, {d_C, m}, {alpha, beta});
    
    // Warmup CUTLASS
    for (int i = 0; i &lt; 10; ++i) {
        cutlass::Status status = gemm_op(args);
        if (status != cutlass::Status::kSuccess) {
            std::cerr &lt;&lt; &quot;CUTLASS GEMM failed: &quot; &lt;&lt; cutlassGetStatusString(status) &lt;&lt; std::endl;
            exit(1);
        }
    }
    CHECK_CUDA(cudaDeviceSynchronize());

    start = std::chrono::high_resolution_clock::now();
    cutlass::Status status = gemm_op(args);
    CHECK_CUDA(cudaDeviceSynchronize());
    end = std::chrono::high_resolution_clock::now();
    std::chrono::duration&lt;float, std::milli&gt; cutlass_time = end - start;

    if (status != cutlass::Status::kSuccess) {
        std::cerr &lt;&lt; &quot;CUTLASS GEMM failed: &quot; &lt;&lt; cutlassGetStatusString(status) &lt;&lt; std::endl;
        exit(1);
    }

    std::cout &lt;&lt; &quot;CUTLASS Time: &quot; &lt;&lt; cutlass_time.count() &lt;&lt; &quot; ms&quot; &lt;&lt; std::endl;

    CHECK_CUDA(cudaMemcpy(h_C_cutlass.data(), d_C, bytes_C, cudaMemcpyDeviceToHost));

    // Verify results
    verify_results(h_C_cublas, h_C_cutlass, m, n);

    // Cleanup
    cublasDestroy(handle);
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);

    return 0;
}</code></pre></details></li></ul><ul id="8ba401f5-da98-4669-8c87-3b5414469c94" class="toggle"><li><details open=""><summary>cuFFT (primarily for convolutions. also audio preprocessing)</summary><ul id="b59aeba1-e635-4c3f-8598-138bf9dc0c19" class="bulleted-list"><li style="list-style-type:disc">fftw stands for the fastest fourier transform in the west (<a href="https://www.fftw.org/">https://www.fftw.org</a>) as a basic FFT cpu C library. cuFFT extends this implementation using everything the same except optimized for the GPU.</li></ul><ul id="59e4e00f-c15a-40cb-87a3-8075398be8c1" class="toggle"><li><details open=""><summary>convolutions</summary><ul id="de13ee48-96c4-4419-8057-d23b0d441117" class="bulleted-list"><li style="list-style-type:disc">convolution modes:<ul id="b0cf3c15-10b6-49e7-8049-25ae9a08f192" class="bulleted-list"><li style="list-style-type:circle">full ⇒ <code>output_size = input_size + kernel_size - 1</code> (best for conv1d)</li></ul><ul id="b26596a6-1f5a-42aa-b4ea-23cbbcca76b5" class="bulleted-list"><li style="list-style-type:circle">valid ⇒ <code>output_size = input_size - kernel_size + 1</code> (we like this one the most for conv2d)</li></ul><ul id="0458d721-6464-4b7b-afcd-67040f238237" class="bulleted-list"><li style="list-style-type:circle">same ⇒ <code>output_size = input_size</code></li></ul></li></ul><ul id="aa11abc0-ee1e-49ea-8e25-ded31eddc9e7" class="bulleted-list"><li style="list-style-type:disc">Convolution backward</li></ul><ul id="fdb0c8a5-63a0-462d-826e-4b5a7dad9a3a" class="bulleted-list"><li style="list-style-type:disc">convolutions w/ CUDA API / manual kernels ⇒ <code>conv_out = ifft( elementwisemul ( fft(x), fft(w) ) )</code></li></ul><ul id="9fccc49a-495d-404e-b618-2a3e3d52c262" class="bulleted-list"><li style="list-style-type:disc">DFT explanation<ul id="9afe9bc3-f0aa-4607-97c2-4b2c3ef2654a" class="bulleted-list"><li style="list-style-type:circle"><code>[1, -1, 1, -1]</code> ⇒ the idea with the DFT is that since the number alternate every 2 indices, you find the frequency bin for k = 2 and make it the maximum possible (the number of samples since its perfectly consistent with k = 2). hence why the output is <code>[0, 0, 4, 0]</code> . we see 4 because <code>N = 4</code> </li></ul><ul id="f76ed28b-4106-4fa9-8ec2-c1dbf131b235" class="bulleted-list"><li style="list-style-type:circle">start with wikipedia definition using practice questions and build from there to show the intuition (<a href="https://en.wikipedia.org/wiki/Discrete_Fourier_transform">https://en.wikipedia.org/wiki/Discrete_Fourier_transform</a>, <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform#:~:text=A%20Fast%20Fourier%20Transform%20">https://en.wikipedia.org/wiki/Fast_Fourier_transform#:~:text=A Fast Fourier Transform </a>(FFT,frequency%20domain%20and%20vice%20versa.)</li></ul><ul id="cef1a955-f781-45e4-942d-f6231788eac7" class="bulleted-list"><li style="list-style-type:circle">compute the DFT/FFT inverse if the intuition of the original calculation is hard.</li></ul></li></ul></details></li></ul><ul id="95ae0967-83ca-489c-9e99-28f9a7fcc3f4" class="toggle"><li><details open=""><summary>doing the DFT by hand then translating to FFT</summary><figure id="dc438c2b-db57-41ae-864b-b6106f9550a9" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2028.png"><img style="width:1824px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2028.png"/></a></figure><figure id="61750fd4-2a0a-44c8-a0e3-5d576d59d315" class="image"><a href="https://cdn.discordapp.com/attachments/1234397662204133427/1247724340762640455/IMG_20240604_192948.jpg?ex=66611158&amp;is=665fbfd8&amp;hm=b0eb9d9141e3bd170536555fea5e189f250048ed1edca5101aaa764932dd6bf5&amp;"><img src="https://cdn.discordapp.com/attachments/1234397662204133427/1247724340762640455/IMG_20240604_192948.jpg?ex=66611158&amp;is=665fbfd8&amp;hm=b0eb9d9141e3bd170536555fea5e189f250048ed1edca5101aaa764932dd6bf5&amp;"/></a></figure></details></li></ul></details></li></ul><ul id="ba18c2de-9817-4748-a0f2-93bb62445f62" class="toggle"><li><details open=""><summary>cuDNN</summary><p id="6fdf857d-3993-4ef3-ac97-30014be0e98f" class="">you technically don’t need cuFFT or a ton of manually written custom kernels to write a GPT training run + inference. fast convolve is built into cuDNN, and cuBLAS matmul is included in cuDNN at greater abstraction. still a good idea to review the idea of slow conv, fast conv, slow matmul, fast matmul.</p><p id="912d15b9-47fb-42af-9953-2fea17688ac4" class="">
</p><p id="25f2fa81-293a-4d0f-9c15-414596d505bc" class="">NVIDIA cuDNN provides highly tuned implementations of operations arising frequently in deep learning applications:</p><ul id="fad0a8b6-ab2c-4308-8ddd-7d73fe0a183b" class="bulleted-list"><li style="list-style-type:disc">Convolution forward and backward including cross-correlation</li></ul><ul id="b9b6b38c-2ce8-4fc0-9465-a9e63e8d6520" class="bulleted-list"><li style="list-style-type:disc">Matrix multiplication</li></ul><ul id="04e8207d-253d-4125-8067-790f95879120" class="bulleted-list"><li style="list-style-type:disc">Pooling forward and backward</li></ul><ul id="83b448e4-18f1-4940-9c43-498258971531" class="bulleted-list"><li style="list-style-type:disc">Softmax forward and backward</li></ul><ul id="9732d51a-1e97-4ac8-aeb6-e18407dbeb05" class="bulleted-list"><li style="list-style-type:disc">Neuron activations forward and backward: relu, tanh, sigmoid, elu, gelu, softplus, swishArithmetic, mathematical, relational, and logical pointwise operations (including various flavors of forward and backward neuron activations)</li></ul><ul id="498f58d7-ea77-452c-be8b-e0b80c4afd7f" class="bulleted-list"><li style="list-style-type:disc">Tensor transformation functions (reshape, transpose, concat, reshape, etc)</li></ul><ul id="e84d27fe-cf27-4a24-9bcb-4002d4a84477" class="bulleted-list"><li style="list-style-type:disc">LRN, LCN, batch normalization, instance normalization, and layer normalization forward and backward</li></ul><p id="f720193d-bcb7-42d7-9866-59007c07dc10" class="">Beyond just providing performant implementations of individual operations, the library also supports a flexible set of multi-operation fusion patterns for further optimization. The goal is to achieve the best available performance on NVIDIA GPUs for important deep learning use cases.</p><p id="c779e638-4df6-45f5-8159-66a6ce260375" class="">In cuDNN version 7 and older, the API was designed to support a fixed set of operations and fusion patterns. We informally call this the “legacy API”. Starting in cuDNN version 8, to address the quickly expanding set of popular fusion patterns, we added a <a href="https://docs.nvidia.com/deeplearning/cudnn/latest/developer/graph-api.html#graph-api">Graph API</a>, which allows the user to express a computation by defining an operation graph, rather than by selecting from a fixed set of API calls. This offers better flexibility versus the legacy API, and for most use cases, is the recommended way to use cuDNN.</p><p id="51fa4c6b-0c62-489a-a890-b38ac5f2c9e8" class="">You may have initially confused the term “Graph API” with operations to do with graph neural networks. It turns out this just lets you define the graph of operations you’d prefer in the form of a Graph. Rather than using fixed operations (legacy API) you can’t actually see code for under the hood (since its a precompiled binary), you get an API you can add to without directly changing the low level source code. </p><p id="7f5f5948-a9b8-4db8-afeb-2c5a20d3a8d6" class="">
</p><p id="91a11371-6128-40ae-a86e-869e35d0b91a" class="">here is the rough idea when it comes to cuDNN docs:<div class="indented"><p id="f5cae63a-2115-4366-9d03-233186adfc85" class="">you have these tensor descriptor types implemented as “opaque struct types” we previously talked about. these descriptors can create tensors, define tensor operations, get attributes about tensors, and more. </p><p id="86daed79-c176-4d89-af39-5540a619d40b" class="">we are going to reverse engineer the following code snippet ( you can type these into google search, find the graph API, and paste the cudnnConvolutionForward to find where the docs for this exist, then map out everything around it and dig into the descriptor types a little more<div class="indented"><p id="4416e693-bb1c-47f3-9c8f-5f55c582a8d3" class=""><code>cudnnTensorDescriptor_t</code></p><p id="4eed65a5-6d6c-4d06-87d1-f0d2e8513e84" class=""><code>cudnnHandle_t</code></p><p id="549282c3-ac49-4b0e-af2f-390b13e8da32" class=""><code>cudnnConvolutionDescriptor_t</code></p><p id="dfb055fc-433b-4281-974e-7b67f8c479dc" class=""><code>cudnnFilterDescriptor_t</code></p><p id="5b5c8471-db7a-4cb8-879f-2e0045d84193" class=""><code>cudnnCreateTensorDescriptor</code><br/><br/><code>cudnnSetTensor4dDescriptor</code></p><p id="a34faeca-d528-4086-8a6a-064b433e4170" class=""><code>cudnnConvolutionFwdAlgo_t</code></p><p id="8cfd0951-945b-4850-9068-0e77b1d31d9f" class="">
</p></div></p><p id="7ed0c820-9141-452a-86c8-38d3cdbb5c2c" class=""><code>cudnnConvolutionForward(cudnn, &amp;alpha, inputDesc, d_input, filterDesc, d_kernel, convDesc, algo, workspace, workspaceSize, &amp;beta, outputDesc, d_output_cudnn)</code></p><p id="76691906-f299-4305-a884-5341b41a3a57" class="">we have a cudnn handle, a pointer to the alpha parameter (not descriptor type), input descriptor, the conv input on device memory, the conv filter/kernel descriptor, the kernel tensor itself, the conv operation descriptor, algo as the forward algorithm type (very top item after clicking on ⇒ <a href="https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-ops-library.html#id172">https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-ops-library.html#id172</a>), the memory workspace the GPU needs to do a conv operation (workspace &amp; workspaceSize), beta is a pointer to a float param, output descriptor, output tensor on device memory.</p><p id="e709c795-5d80-42ba-8efd-7c7648efdba4" class="">you want cudnn to take in your input tensor as</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="c54bef44-f81a-4bcd-b400-fe6ad9359a27" class="code"><code class="language-JavaScript">tensor([[[-1.7182,  1.2014, -0.0144],
         [-0.6332, -0.5842, -0.7202]],

        [[ 0.6992, -0.9595,  0.1304],
         [-0.0369,  0.8105,  0.8588]],

        [[-1.0553,  1.9859,  0.9880],
         [ 0.6508,  1.4037,  0.0909]],

        [[-0.6083,  0.4942,  1.9186],
         [-0.7630, -0.8169,  0.6805]]])</code></pre><p id="18da1efe-34ed-4a52-a20f-724af6b032c5" class="">as a pytorch reference. but want you allocate memory its just a <code>&lt;vector&gt;</code> of int/floats. </p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="8156a626-eb98-4cfe-bedc-acf7f4e91567" class="code"><code class="language-JavaScript">[-1.7182,  1.2014, -0.0144, -0.6332, -0.5842, -0.7202,  0.6992, -0.9595,
	0.1304, -0.0369,  0.8105,  0.8588, -1.0553,  1.9859,  0.9880,  0.6508,
	1.4037,  0.0909, -0.6083,  0.4942,  1.9186, -0.7630, -0.8169,  0.6805])</code></pre><p id="39b194ce-f44a-404d-afc2-debbbd698977" class="">it turns out this part isn’t as bad as you would expect. notice how we have the shape (4, 2, 3). we can split into 4 equal sections (our batch elements), split each of those into 2 sections (maybe time dimension), at this point we are left with the original intended shape. this is how cudnn handles your tensors under the hood. as long as you specify the shape properly (ex: NCHW ⇒ batch_size, channels, height, width) you have nothing to worry about (still cudnn error check of course)</p><p id="61123e4e-20a2-4ca8-9173-0889bbf29903" class="">
</p><p id="6c7e9a7a-ed60-48a3-b870-4d21ca5d3c8f" class="">all code I used here is below</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="eab0b59c-49c1-45a7-b3fe-2bf7c6c7381c" class="code"><code class="language-JavaScript">    float *d_input, *d_kernel, *d_output_custom, *d_output_cudnn;
    cudaMalloc(&amp;d_input, B*C*H*W*sizeof(float));
    cudaMalloc(&amp;d_kernel, K*C*KH*KW*sizeof(float));
    cudaMalloc(&amp;d_output_custom, B*K*H*W*sizeof(float));
    cudaMalloc(&amp;d_output_cudnn, B*K*H*W*sizeof(float));

    // Copy input and kernel to device
    cudaMemcpy(d_input, h_input, B*C*H*W*sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_kernel, h_kernel, K*C*KH*KW*sizeof(float), cudaMemcpyHostToDevice);

    

    // Kernel launch configuration
    dim3 blockDim(H, W);
    dim3 gridDim(B, K);
    custom_conv2d_kernel&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;(d_input, d_output_custom, d_kernel, B, C, H, W, K, KH, KW);

    // Copy custom kernel output back to host
    cudaMemcpy(h_output_custom, d_output_custom, B*K*H*W*sizeof(float), cudaMemcpyDeviceToHost);

    // cuDNN setup
    cudnnHandle_t cudnn;
		  cudnnCreate(&amp;cudnn);
		
		cudnnTensorDescriptor_t inputDesc, outputDesc;
		cudnnFilterDescriptor_t filterDesc;
		cudnnConvolutionDescriptor_t convDesc;
		cudnnCreateTensorDescriptor(&amp;inputDesc);
    cudnnCreateTensorDescriptor(&amp;outputDesc);
    cudnnCreateFilterDescriptor(&amp;filterDesc);
    cudnnCreateConvolutionDescriptor(&amp;convDesc);

    cudnnSetTensor4dDescriptor(inputDesc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, B, C, H, W);
    cudnnSetFilter4dDescriptor(filterDesc, CUDNN_DATA_FLOAT, CUDNN_TENSOR_NCHW, K, C, KH, KW);
    cudnnSetConvolution2dDescriptor(convDesc, 0, 0, 1, 1, 1, 1, CUDNN_CONVOLUTION, CUDNN_DATA_FLOAT);

    int n, c, h, w;
    cudnnGetConvolution2dForwardOutputDim(convDesc, inputDesc, filterDesc, &amp;n, &amp;c, &amp;h, &amp;w);
    cudnnSetTensor4dDescriptor(outputDesc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, n, c, h, w);

    cudnnConvolutionFwdAlgo_t algo;
    cudnnGetConvolution2dForwardOutputDim(convDesc, inputDesc, filterDesc, &amp;n, &amp;c, &amp;h, &amp;w);

    size_t workspaceSize;
    cudnnGetConvolutionForwardWorkspaceSize(cudnn, inputDesc, filterDesc, convDesc, outputDesc, algo, &amp;workspaceSize);

    void* workspace;
    cudaMalloc(&amp;workspace, workspaceSize);

    const float alpha = 1.0f, beta = 0.0f;
    CUDNN_CHECK(cudnnConvolutionForward(cudnn, &amp;alpha, inputDesc, d_input, filterDesc, d_kernel, convDesc, algo, workspace, workspaceSize, &amp;beta, outputDesc, d_output_cudnn));</code></pre><p id="224010ec-0135-4bd0-9d77-5e1e12326696" class="">
</p><p id="76fb4815-bc75-4b60-ba06-a43a16f2b95c" class="">
</p><p id="4c9b7378-6681-424e-bb31-9a9766899cd8" class="">
</p></div></p><ol type="1" id="75924619-f848-4e94-b787-bc499874eaca" class="numbered-list" start="1"><li><strong>Pre-compiled Single Operation Engines</strong>:<ul id="19c75e5f-b94e-4de7-8be0-0ffc554ed183" class="bulleted-list"><li style="list-style-type:disc">These engines are pre-compiled and optimized for a specific single operation. Because they are pre-compiled, they offer very efficient execution but are inflexible in terms of the operations they can perform.</li></ul><ul id="11f64d82-3e7e-479d-a8fc-e3f4e4adebb7" class="bulleted-list"><li style="list-style-type:disc">Example: A matrix multiplication engine that is pre-compiled and optimized specifically for that operation.</li></ul></li></ol><ol type="1" id="3df89e02-8dd7-4a55-8c91-d8699474e1d5" class="numbered-list" start="2"><li><strong>Generic Runtime Fusion Engines</strong>:<ul id="d917e088-f2b6-4502-befd-782232f52101" class="bulleted-list"><li style="list-style-type:disc">These engines are designed to dynamically fuse multiple operations at runtime. They offer more flexibility compared to pre-compiled engines since they can adapt to different combinations of operations but might not be as highly optimized as pre-compiled or specialized runtime engines.</li></ul><ul id="9efffb47-ef6d-4cfe-86d8-2e31113474c2" class="bulleted-list"><li style="list-style-type:disc">Example: An engine that can dynamically fuse different element-wise operations on tensors during execution to avoid redundant memory reads/writes. (you can fuse uncommon operations together, gaining a decent improvement, but still not as fast as pre-compiled).</li></ul></li></ol><ol type="1" id="0cb33024-7335-4388-a8fa-e072fa078f5d" class="numbered-list" start="3"><li><strong>Specialized Runtime Fusion Engines</strong>:<ul id="d0a7d61c-d633-48fd-b810-0f1edaeb4324" class="bulleted-list"><li style="list-style-type:disc">Similar to generic runtime fusion engines, but these are specifically optimized for certain patterns or combinations of operations. They still offer runtime flexibility but also try to leverage optimizations for particular use cases or operation sequences.</li></ul><ul id="d1a43a42-e6b3-4cf0-ab43-b97cec30779d" class="bulleted-list"><li style="list-style-type:disc">Example: An engine optimized for fusing convolutional layers followed by activation functions in neural networks. It will recognize your code architecture or some pattern during the CUDA script compilation and find the fused operations in the backend where you would get a speedup</li></ul></li></ol><ol type="1" id="fcc3f79f-2aeb-46ef-8f21-4a3eefc90ba0" class="numbered-list" start="4"><li><strong>Specialized Pre-compiled Fusion Engines</strong>:<ul id="6519bc1a-b447-46fa-a6d5-70b010eebe3f" class="bulleted-list"><li style="list-style-type:disc">These engines are pre-compiled and optimized for specific sequences of operations. They offer the same high performance as pre-compiled single operation engines but can handle sequences of operations rather than just single ones.</li></ul><ul id="7150677e-95b0-4597-b7af-b5dd7d5fa6e0" class="bulleted-list"><li style="list-style-type:disc">Example: A pre-compiled engine for a specific convolutional block in a neural network that combines convolution, batch normalization, and ReLU activation functions.</li></ul></li></ol><p id="bf6635d5-95a7-4487-8ce5-e5fb7cb0695e" class="">
</p><h3 id="b889a038-edde-40ca-808e-71c982a153ab" class="">Runtime Fusion:</h3><p id="c80ef5ac-bcae-4f1e-af4d-d2fde60bb176" class="">Consider a scenario where you need to perform several element-wise operations on tensors, such as addition, multiplication, and a sigmoid activation function. Without runtime fusion, each operation would be a separate kernel launch, each reading from and writing to global memory:</p><p id="6435930c-ed57-4dd1-bfe5-a86e611a0f65" class=""><code>output = torch.sigmoid(tensor1 + tensor2 * tensor3)</code></p><p id="3184bf01-afb8-4249-8f0a-7dbec25b6925" class="">With runtime fusion, the above operations could be combined into a single kernel launch, thus performing the entire computation in one go, keeping intermediate results in registers and only writing the final output to global memory.</p><h2 id="ffc8fb01-d79b-47a6-b8f5-ce879a78289b" class="">Graph API</h2><ul id="48eb8d79-ae3f-46de-96ce-1f2c8a262498" class="bulleted-list"><li style="list-style-type:disc"><a href="https://docs.nvidia.com/deeplearning/cudnn/latest/developer/graph-api.html">https://docs.nvidia.com/deeplearning/cudnn/latest/developer/graph-api.html</a></li></ul><ul id="125e6f77-1581-4b6b-a565-06c145e56494" class="bulleted-list"><li style="list-style-type:disc">Of course, for fusion to be interesting, the graph needs to support multiple operations. And ideally, we want the supported patterns to be flexible to cover a diverse set of use cases. To accomplish this generality, cuDNN has runtime fusion engines that generate the kernel (or kernels) at runtime based on the graph pattern. This section outlines the patterns supported by these runtime fusion engines (that is, engines with <code>CUDNN_BEHAVIOR_NOTE_RUNTIME_COMPILATION</code> behavioral note).</li></ul><figure id="76575191-6942-4f62-8d82-abe627c278e6" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2029.png"><img style="width:842px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2029.png"/></a></figure><figure id="8e9a6368-f81e-42e0-8db3-facacf6a9020" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2030.png"><img style="width:1136px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2030.png"/></a></figure><p id="b352eeb5-2d64-41c6-9c67-0bdc3a469c3c" class="">you will have to check the compute compatibility of your GPU to see which of these operations will fuse</p><h2 id="1c04c9e5-d8b9-4c9e-a4d4-777321f781a7" class="">Ops (operations) API</h2><p id="ed5b7032-19ff-4365-aade-3bde01a3503a" class="">
</p><h2 id="f2434742-cf96-4a7b-b5c7-a8de8849b218" class="">CNN (convolutional neural net) API</h2><p id="aa0cc369-7362-429c-a612-25b0c61dd39c" class="">
</p><h2 id="483237b2-4764-4235-8155-d67acf8a7a50" class="">Adversarial API</h2><p id="3195a1c2-7216-471b-b56e-fe261e58e50d" class="">
</p></details></li></ul><ul id="0ec00475-4e68-4fde-8260-818558c2d109" class="toggle"><li><details open=""><summary>NCCL (NVIDIA Collective Communications Library ⇒ for dist cluster computing)</summary><p id="d57b4662-f138-4b12-b640-de1b593993ca" class="">distributed computing — useful if you’re building datacenters :D</p><p id="c0a6aced-aa63-4942-9630-5086caaf1a3f" class="">in pytorch, you’d use Distributed Data Parallel ⇒ <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html</a>, but if you’re writing GPT-5’s training run you’d want to pay CUDA experts to squeeze every bit of performance out at the datacenter level.</p><p id="39102b01-789c-4836-99ba-dc67964e876d" class=""><a href="https://www.youtube.com/watch?v=T22e3fgit-A&amp;ab_channel=CUDAMODE">https://www.youtube.com/watch?v=T22e3fgit-A&amp;ab_channel=CUDAMODE</a></p><p id="e9540559-4c01-4d35-8f18-efb1a774e5e9" class="">Model Parallelism (weights) VS Data parallelism (batches)</p><p id="7dd59332-19ed-4c86-af50-67e52f99d8bc" class="">
</p><p id="be858d34-c8d9-4665-8395-b4d7fcb2968b" class="">setup here ⇒ <a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html">https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html</a></p><p id="2e225518-52fe-48ae-872e-d507915dbd13" class="">we can see all the operations here ⇒ <a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api.html">https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api.html</a></p><p id="5bc7ab13-7479-467a-9137-03a143de4e28" class="">scroll to the next page for descriptions on these API functions</p></details></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.5em;line-height:1.3;margin:0">Make the FFT (opt.) and MatMul custom kernels faster</summary><div class="indented"><ul id="c7f839de-6952-441f-83ac-fce1856d2bc1" class="bulleted-list"><li style="list-style-type:disc">not just speed, but also memory usage (larger mem requirement for transformer architecture)<ul id="7cfba4c8-277f-4b56-b77a-927a06710f26" class="bulleted-list"><li style="list-style-type:circle">time complexity reduction of parallelizing a matmul so each core gets a <code>O(n)</code> dot product operation instead of quadratic (or higher). show the dot product vs full matmul time complexity my hand</li></ul></li></ul><ul id="cf97fd23-deda-48fb-8210-d057497b3683" class="bulleted-list"><li style="list-style-type:disc">use dynamic parallelism to speed up sub-jobs<ul id="8e248907-3ef8-440b-ae3e-102ce58dfc3f" class="bulleted-list"><li style="list-style-type:circle">launch <code>__device__</code> kernel inside a <code>__global__</code> kernel</li></ul></li></ul><ul id="7f1aa82d-4680-4330-9d5e-a1deb65956aa" class="bulleted-list"><li style="list-style-type:disc">use unified memory to have the <code>nvcc</code> compiler manage stuff for us (shared mem between cpu and gpu)</li></ul><ul id="269b9e90-c238-41c6-a8a1-7a681588eae3" class="bulleted-list"><li style="list-style-type:disc">performance profiling w/ <code>nvprof</code> and <code>nsys</code></li></ul><ul id="a03f5e5e-745d-475d-bf57-aacad3c2f783" class="bulleted-list"><li style="list-style-type:disc">introduce the debugger to show everything working as expected (small size data structures that are human readable)</li></ul><ul id="c562d38c-d2e7-4b84-be29-a1dbfd30aa5a" class="bulleted-list"><li style="list-style-type:disc">profile with respect to CUTLASS and CUBLAS</li></ul><figure id="fe58ab25-94eb-45e2-98d3-c512c366848b" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2031.png"><img style="width:677px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2031.png"/></a></figure><ul id="8f0ebea2-5692-470a-a848-2a042217d2f7" class="toggle"><li><details open=""><summary>MATMUL</summary><ul id="af235388-96e1-4fec-8a90-953e11d24f0b" class="bulleted-list"><li style="list-style-type:disc"><a href="https://siboehm.com/articles/22/CUDA-MMM">https://siboehm.com/articles/22/CUDA-MMM</a></li></ul><ul id="c37b9582-6ca0-417b-ab6c-7eb6606ca800" class="bulleted-list"><li style="list-style-type:disc">python (pytorch/numpy) ⇒ break it up into a slow python function</li></ul><ul id="79910ef3-abac-4935-9976-f534d03cd710" class="bulleted-list"><li style="list-style-type:disc">rewrite for CPU in C</li></ul><ul id="9aef6700-f532-4cad-820a-51bbc3546678" class="bulleted-list"><li style="list-style-type:disc">naive kernel</li></ul><ul id="4885c158-841c-42ea-910c-536b1a0b0756" class="bulleted-list"><li style="list-style-type:disc">with tiling ⇒ <a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html">https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html</a></li></ul><ul id="75bbc7e2-cb50-48fb-8e4b-8c44adb2d9be" class="bulleted-list"><li style="list-style-type:disc">plus shared memory</li></ul><ul id="b35c5bb0-09a4-4e71-b3fe-03c11b1a6ea0" class="bulleted-list"><li style="list-style-type:disc">etc…</li></ul><ul id="333d1caa-723b-4b54-a380-f27ea1880c01" class="bulleted-list"><li style="list-style-type:disc">cuBLAS (SOTA)</li></ul></details></li></ul><ul id="d18f5fbe-d96f-46dc-b9fe-8370ddb59afa" class="toggle"><li><details open=""><summary>FFT (or CONV)</summary><ul id="f8149584-5a71-4aea-89e9-6734cf4beb46" class="bulleted-list"><li style="list-style-type:disc">python (numpy)</li></ul><ul id="f904b211-97d7-4c68-85ee-3929c3f2d146" class="bulleted-list"><li style="list-style-type:disc">rewrite for CPU in C</li></ul><ul id="1253099f-51ca-4db6-bee9-ceaddfd48403" class="bulleted-list"><li style="list-style-type:disc">naive kernel</li></ul><ul id="94db6ac5-750a-4775-9570-0cc7bf7c8a45" class="bulleted-list"><li style="list-style-type:disc">efficient recursion</li></ul><ul id="9604ee6b-98b5-49c6-b4f5-81520db2815d" class="bulleted-list"><li style="list-style-type:disc">tiling + shared</li></ul><ul id="1b4d3ea5-b99d-4eed-9207-83b18abeff06" class="bulleted-list"><li style="list-style-type:disc">etc…</li></ul></details></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.5em;line-height:1.3;margin:0">PyTorch extensions w/ backprop compatibility</summary><div class="indented"><ul id="6e401f31-63b9-4275-be4f-744b263fe676" class="bulleted-list"><li style="list-style-type:disc">side notes: you may see cool functions like flash attention as <code>scaled_dot_product_attention</code> in the pytorch docs. when it says its supported by architecture better than or equal to ampere architecture, its not referring to just A100s, its any GPU with the ampere architecture. You can do a google search of your GPU architecture. for example, my RTX 3070 is ampere.</li></ul><ul id="bd0d8dcf-3384-4dcb-80d7-f86fec7de073" class="bulleted-list"><li style="list-style-type:disc">differentiable polynomial function (if backprop is required include it)</li></ul><ul id="c899b665-0652-4a2d-ba8a-6e286d76d85d" class="bulleted-list"><li style="list-style-type:disc">showcase and use micrograd cuda (don’t redo from scratch)<ul id="91c4e3fc-450c-432c-bb43-f3d2a8b79322" class="bulleted-list"><li style="list-style-type:circle">micrograd ⇒ micrograd cuda<ul id="9ac8d01c-004e-4f65-9b97-c9fe21375f91" class="bulleted-list"><li style="list-style-type:square">neurons</li></ul><ul id="8b90b55e-83a2-4875-8f78-022566d8ac7a" class="bulleted-list"><li style="list-style-type:square">layers</li></ul><ul id="fff547c0-2a16-4983-98cb-09a88a355817" class="bulleted-list"><li style="list-style-type:square">MLPs<ul id="b8c1a592-3372-461c-bbb7-c401429e4e47" class="bulleted-list"><li style="list-style-type:disc">forward</li></ul><ul id="a9d37079-e48d-429e-a38a-99d33ff0d4f5" class="bulleted-list"><li style="list-style-type:disc">backward</li></ul><ul id="6335a7f5-dda5-49ff-94c2-803aabdb5f03" class="bulleted-list"><li style="list-style-type:disc">activations</li></ul><ul id="dafcd1ea-0f2a-485d-a0a1-1953e3a95eec" class="bulleted-list"><li style="list-style-type:disc">loss functions</li></ul></li></ul><ul id="04bd8639-c6b1-41e3-ab22-b126585d3a59" class="bulleted-list"><li style="list-style-type:square">if <code>wx.grad</code> (prev node) and <code><a href="http://x.data">x.data</a></code> are large (large input data and the next function has a large impact on the loss), you will want to nudge the <code><a href="http://w.data">w.data</a></code> in the opposite direction more than a smaller change would require. you do this by storing <code>w.grad = x.data * wx.grad</code>. this should remind you of the chain rule and how a large error will propagate forward as its multiplied by bigger numbers.</li></ul></li></ul></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.5em;line-height:1.3;margin:0">More resources + next steps</summary><div class="indented"><ul id="0d144189-1053-4c95-b1e7-ef5616f73ee6" class="bulleted-list"><li style="list-style-type:disc">research areas<ul id="338fd842-78c2-4fbe-9cda-3207a722d445" class="bulleted-list"><li style="list-style-type:circle">quantization</li></ul><ul id="d2cc905b-f4f2-463f-9028-d4fedb2ea1a2" class="bulleted-list"><li style="list-style-type:circle">hardware architecture</li></ul><ul id="a125e335-eaea-43f7-aed6-3c7a20892bb0" class="bulleted-list"><li style="list-style-type:circle">sparsity</li></ul></li></ul><ul id="3e8f914c-710f-4c21-902d-df2f84bdbe9c" class="bulleted-list"><li style="list-style-type:disc">how to approach reading the other docs</li></ul><ul id="354d6f61-eb3a-44fb-bda2-7f6a9f4497cd" class="bulleted-list"><li style="list-style-type:disc">other cuda software (desktop tools)</li></ul><ul id="c33f5891-c31f-4e4f-a5c2-94f41de0014b" class="bulleted-list"><li style="list-style-type:disc">micrograd ⇒ <a href="https://github.com/karpathy/micrograd">https://github.com/karpathy/micrograd</a></li></ul><ul id="147adb9e-5c86-4105-a3e1-897e05bce487" class="bulleted-list"><li style="list-style-type:disc">micrograd_cuda ⇒ <a href="https://github.com/mlecauchois/micrograd-cuda">https://github.com/mlecauchois/micrograd-cuda</a></li></ul><ul id="ad478cc5-1e41-4b50-b633-8c38deb53891" class="bulleted-list"><li style="list-style-type:disc">CUDA MODE ⇒ <a href="https://github.com/cuda-mode/lectures">https://github.com/cuda-mode/lectures</a></li></ul><ul id="e223f921-aa12-4029-86cb-57a3d0be0076" class="bulleted-list"><li style="list-style-type:disc">similar to the final project ⇒ <a href="https://github.com/haanjack/mnist-cudnn">https://github.com/haanjack/mnist-cudnn</a></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.5em;line-height:1.3;margin:0">For those who just want to squeeze out speed without learning a ton</summary><div class="indented"><ul id="24171a70-aced-4ee1-9e61-9dbae9a022f1" class="bulleted-list"><li style="list-style-type:disc">pytorch (<code>torch.compile</code> for speed) ⇒ <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html</a></li></ul><ul id="ef400f98-863c-423f-a1b8-c9c6e7c2cf7f" class="bulleted-list"><li style="list-style-type:disc">vLLM</li></ul><p id="1951bc14-ca73-4ee1-a82d-7ce294e99085" class="">
</p><h3 id="b4ec96ef-7c8f-4faa-88b3-7addbceb1129" class="">Community Recommendations:</h3><figure id="d8834498-e2a3-42cc-8607-25fe07f35c7a" class="image"><a href="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2032.png"><img style="width:1596px" src="CUDA%20Course%20V2%20a67249b9e99c49d283af61f02d52f186/Untitled%2032.png"/></a></figure><p id="678e26c0-569d-4dc0-82d5-57a09a8f2a91" class="">
</p><h3 id="c572a5f1-0776-422e-967b-e546a5a5b778" class="">Unsorted:</h3><ul id="4da5072a-c149-4f52-9b2a-3f8ade2ee88c" class="toggle"><li><details open=""><summary>what is thread safety?</summary><p id="0868b72b-b36e-4c0b-9e23-d8e80f595bdc" class=""><a href="https://forums.developer.nvidia.com/t/is-cuda-thread-safe/2262">https://forums.developer.nvidia.com/t/is-cuda-thread-safe/2262</a></p><p id="34247d8d-dbc4-450e-8070-4f6bc6cd08c9" class="">when a piece of code is “thread-safe” it can be run by multiple threads at the same time without leading to race conditions or other unexpected behaviour.</p><p id="aa9a93bd-ac93-4920-8832-92d719b67e60" class="">race conditions are where one thread starts the next task before another finishes. to prevent race conditions, we use a special function called <code>cudaDeviceSynchronize()</code> to ensure all threads are caught up before giving them a new instruction to work on. think about a bunch of threads racing to the finish line, some finish before others for some reason and you have to manually tell those “winner” threads to wait at the finish line for the laggards.</p><p id="2b9a8117-ca16-4afc-bcaf-fdbed5bbc8ee" class="">if you are wondering about calling multiple GPU kernels with different CPU threads, refer to the link above.</p></details></li></ul><ul id="237ce510-a03d-4ef3-9175-d4927364cbc3" class="toggle"><li><details open=""><summary>what are atomic operations</summary><p id="5294a20c-3702-4318-96dd-8c97a18c07ff" class="">by “atomic” we are referring to the indivisibility concept in physics where a thing cannot be broken down further.</p><p id="a5fe524a-598e-4b05-afde-507152a6cad1" class="">An <strong>atomic operation</strong> ensures that a particular operation on a memory location is completed entirely by one thread before another thread can access or modify the same memory location. This prevents race conditions.</p><p id="3aa5f778-fd63-4504-a66d-787c5088c1a1" class="">Since we limit the amount of work done on a single piece of memory per unit time throughout an atomic operation, we lose slightly to speed. It is hardware guaranteed to be memory safe at a cost of speed.</p><h3 id="d90af247-c352-4e39-8fd9-8c2dd26af72b" class=""><strong>Integer Atomic Operations</strong></h3><ul id="bc0a8982-d1d0-4a5f-af0b-0c42c35adb62" class="bulleted-list"><li style="list-style-type:disc"><code><strong>atomicAdd(int* address, int val)</strong></code>: Atomically adds <code>val</code> to the value at <code>address</code> and returns the old value.</li></ul><ul id="3e53d6ae-6b57-40d2-9a6c-b0fea6a28f1b" class="bulleted-list"><li style="list-style-type:disc"><code><strong>atomicSub(int* address, int val)</strong></code>: Atomically subtracts <code>val</code> from the value at <code>address</code> and returns the old value.</li></ul><ul id="3873d77c-eafa-42b5-86a1-d6b8ad15f0d5" class="bulleted-list"><li style="list-style-type:disc"><code><strong>atomicExch(int* address, int val)</strong></code>: Atomically exchanges the value at <code>address</code> with <code>val</code> and returns the old value.</li></ul><ul id="aea256e4-6ab1-498c-aa21-aa44b7594504" class="bulleted-list"><li style="list-style-type:disc"><code><strong>atomicMax(int* address, int val)</strong></code>: Atomically sets the value at <code>address</code> to the maximum of the current value and <code>val</code>.</li></ul><ul id="e738ea42-755f-4d39-b4f2-64483db008ec" class="bulleted-list"><li style="list-style-type:disc"><code><strong>atomicMin(int* address, int val)</strong></code>: Atomically sets the value at <code>address</code> to the minimum of the current value and <code>val</code>.</li></ul><ul id="caa15d16-3cf5-455f-8fbe-25d8628ebfaf" class="bulleted-list"><li style="list-style-type:disc"><code><strong>atomicAnd(int* address, int val)</strong></code>: Atomically performs a bitwise AND of the value at <code>address</code> and <code>val</code>.</li></ul><ul id="15a8a696-464d-4c6f-a9a8-0140a599173c" class="bulleted-list"><li style="list-style-type:disc"><code><strong>atomicOr(int* address, int val)</strong></code>: Atomically performs a bitwise OR of the value at <code>address</code> and <code>val</code>.</li></ul><ul id="0e79b458-e283-40ec-8f25-85fb8a0507ff" class="bulleted-list"><li style="list-style-type:disc"><code><strong>atomicXor(int* address, int val)</strong></code>: Atomically performs a bitwise XOR of the value at <code>address</code> and <code>val</code>.</li></ul><ul id="1b759581-85db-4dc0-850f-783db93a5639" class="bulleted-list"><li style="list-style-type:disc"><code><strong>atomicCAS(int* address, int compare, int val)</strong></code>: Atomically compares the value at <code>address</code> with <code>compare</code>, and if they are equal, replaces it with <code>val</code>. The original value is returned.</li></ul><h3 id="339d4f3b-d143-464f-916e-a8e74bc4ebab" class=""><strong>Floating-Point Atomic Operations</strong></h3><ul id="9cfcc76e-fe58-4cf9-a447-8d1c095908d7" class="bulleted-list"><li style="list-style-type:disc"><code><strong>atomicAdd(float* address, float val)</strong></code>: Atomically adds <code>val</code> to the value at <code>address</code> and returns the old value. Available from CUDA 2.0.</li></ul><ul id="c450b83f-bf06-480d-9d68-de6499a7cddd" class="bulleted-list"><li style="list-style-type:disc">Note: Floating-point atomic operations on double precision variables are supported starting from CUDA Compute Capability 6.0 using <code>atomicAdd(double* address, double val)</code>.</li></ul><ul id="bd562437-850c-4110-8da2-0356893ff38a" class="toggle"><li><details open=""><summary>vLLM</summary></details></li></ul></details></li></ul><ul id="25435fa0-efb7-4c95-afbb-9d9aa30505e4" class="toggle"><li><details open=""><summary>vLLM</summary><p id="5cee9450-6d07-4a23-b93c-1cebdef0830a" class=""><a href="https://github.com/vllm-project/vllm/">https://github.com/vllm-project/vllm/</a> is a pip library to run highly efficient versions of finetuned models in inference mode. They implement all the latest greatest features from deep learning CUDA kernels.</p></details></li></ul><ul id="c93f0647-1e7c-4f03-ad2d-4b50f0a351bf" class="toggle"><li><details open=""><summary>Is it correct to say a lot of the CUDA kernels we use are auto generated rather than purely human written?</summary><ul id="12a8b2c0-ee7a-49f1-b2cf-b0ad62b1d6ec" class="bulleted-list"><li style="list-style-type:disc">kernel fusion does exactly this. <code>y = w * x + b</code> instead of doing seperate operations for this and doing excessive memory allocation and copying, CUDA will be smart when we give it permission to look for fusion opportunities and will compress multi-step operations into a single, highly optimized operation during compilation</li></ul><ul id="d57b2e12-d0e5-4b32-ad75-f510c4d2ef82" class="bulleted-list"><li style="list-style-type:disc">NVIDIA thrust is an autogenerator but used for other parallel algorithms we don’t see in deep learning</li></ul></details></li></ul><ul id="fa5edcc1-f2d9-46b7-86cd-97867c73c8bb" class="toggle"><li><details open=""><summary>if/else is slower from path divergence</summary><ul id="52397595-18c2-49e9-abbc-a54e07687a63" class="bulleted-list"><li style="list-style-type:disc">vector addition is fast because divergence isn’t possible, not a different possible way for instructions to carry out.</li></ul><ul id="5022d2cb-92b2-48fb-b02c-91b934dff451" class="bulleted-list"><li style="list-style-type:disc">if you have an if + else statement in a cuda kernel, the controller will do all the indices of <code>if</code> first, then <code>else</code> after all the ifs are done. more branches = more kernel calls</li></ul></details></li></ul><ul id="9138168a-7603-4bcf-848b-ee5792faa9cf" class="toggle"><li><details open=""><summary>efficient transpose <code>A.T</code></summary><p id="370ffad3-8c05-45da-9701-d7d841acd9f8" class="">coalesced ⇒ come together to form one mass or whole</p><ul id="cc4c1903-8086-462c-b66b-0398b55d5c8e" class="bulleted-list"><li style="list-style-type:disc">shared memory</li></ul><ul id="58e24cc7-00d9-4a8e-9fb9-5f53460574ea" class="bulleted-list"><li style="list-style-type:disc">tiling</li></ul><ul id="37de7e50-6e48-40ae-9071-38ac93524fdb" class="bulleted-list"><li style="list-style-type:disc">memory efficiency</li></ul><ul id="cfd748e2-9b27-4086-88d4-b2d8491333a6" class="bulleted-list"><li style="list-style-type:disc"><a href="https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/">https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/</a></li></ul></details></li></ul><ul id="7586e2db-7318-4339-9f61-12bc1dde77e2" class="toggle"><li><details open=""><summary>What is Fusing?</summary><ul id="f43504ae-67a6-479e-aa9d-07a786212510" class="bulleted-list"><li style="list-style-type:disc">fusing is when you put many functions inside a bigger one<ul id="2a20d4f9-377b-419f-a983-c8bf932ae551" class="bulleted-list"><li style="list-style-type:circle">some of you might think that this is to reduce the number of kernel launches since they supposed take a while to start and execute during runtime. You can do at most 200,000 kernel launches per second (<a href="https://forums.developer.nvidia.com/t/any-way-to-measure-the-latency-of-a-kernel-launch/221413">https://forums.developer.nvidia.com/t/any-way-to-measure-the-latency-of-a-kernel-launch/221413</a>)</li></ul><ul id="8bb17107-0dae-4382-b34b-ae6b15176b0e" class="bulleted-list"><li style="list-style-type:circle">then what is fusing for?<ul id="e05fbb60-548a-4136-97c4-f2a3812cfc31" class="bulleted-list"><li style="list-style-type:square">just to reduce the amount of things you need to manage in the calling process. for example here we combine all the operations into a single kernel by “fusing” them</li></ul><ul id="7f097b2f-9de8-455d-a35f-749850f961b4" class="bulleted-list"><li style="list-style-type:square">without kernel fusing ⇒ conv, batchnorm, pooling, activations each require a seperate kernel launch</li></ul><ul id="22c64c79-b7a6-4dc1-8af5-f793643f8528" class="bulleted-list"><li style="list-style-type:square">with kernel fusing ⇒ conv, batchnorm, pooling, activations are all launches in one kernel with efficient parallel design to run fast</li></ul><ul id="a87484f5-b44b-405f-93c8-062dc81b0051" class="bulleted-list"><li style="list-style-type:square">main bottleneck here is the launch time of the 2 other kernels.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="12c94ef9-c2be-45ef-afaf-bbf75dabb3ea" class="code"><code class="language-C">// Example with fused operations, assuming a more realistic neural network forward pass.
__global__ void fusedConvBatchNormReLUPool(float* input, float* output, float* weights, float* batchNormParams, int N) {
    // Block index
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx &lt; N) {
        // Convolution (simplified, generally more complex in real scenarios)
        float convResult = 0.0f;
        for (int k = 0; k &lt; KERNEL_SIZE; ++k) {
            convResult += input[idx + k] * weights[k];
        }

        // Batch Normalization
        float normalized = (convResult - batchNormParams[0]) / batchNormParams[1];   // Simplified

        // ReLU Activation
        normalized = fmaxf(0.0f, normalized);

        // Pooling (again simplified)
        if (idx % 2 == 0) { // Example pooling condition
            output[idx / 2] = fmaxf(normalized, input[idx + 1]); // Max pooling
        }
    }
}

int main() {
    // Allocate and initialize device memory for input, output, weights, and batchNorm parameters
    float* d_input, *d_output, *d_weights, *d_batchNormParams;
    cudaMalloc(&amp;d_input, N * sizeof(float));
    cudaMalloc(&amp;d_output, (N/2) * sizeof(float));  // assuming pooling halves the output size
    cudaMalloc(&amp;d_weights, KERNEL_SIZE * sizeof(float));
    cudaMalloc(&amp;d_batchNormParams, 2 * sizeof(float)); // mean and variance for batch norm
    
    // Launch the fused kernel
    int blockSize = 256;
    int numBlocks = (N + blockSize - 1) / blockSize;
    fusedConvBatchNormReLUPool&lt;&lt;&lt;numBlocks, blockSize&gt;&gt;&gt;(d_input, d_output, d_weights, d_batchNormParams, N);
    
    // Cleanup
    cudaFree(d_input);
    cudaFree(d_output);
    cudaFree(d_weights);
    cudaFree(d_batchNormParams);
    
    return 0;
}</code></pre></li></ul></li></ul></details></li></ul><h3 id="b89a4ada-b0a6-4386-b2fe-5fe5a454a5d4" class="">Uncovered</h3><ul id="07102559-665f-4e11-8ada-c66f12731bd9" class="bulleted-list"><li style="list-style-type:disc">what is triton?<ul id="1ccf0101-cf96-47f8-bd6d-60a27ed3b17e" class="bulleted-list"><li style="list-style-type:circle"><a href="https://openai.com/index/triton/">https://openai.com/index/triton/</a></li></ul><ul id="0d537b68-0590-4440-a3c8-45e7af974fb5" class="bulleted-list"><li style="list-style-type:circle"><a href="https://github.com/triton-lang/triton">https://github.com/triton-lang/triton</a></li></ul><ul id="9030f832-8896-4b1b-a9f8-7bcfc6ed431c" class="bulleted-list"><li style="list-style-type:circle"><a href="https://triton-lang.org/main/index.html">https://triton-lang.org/main/index.html</a></li></ul></li></ul><ul id="2de3dd8d-add2-4b27-8efa-966a29b62f75" class="bulleted-list"><li style="list-style-type:disc">reddit recommends: torch ⇒ torch.compile ⇒ triton ⇒ CUDA</li></ul><ul id="df1d541f-3b88-44bc-b463-8d773e09b389" class="bulleted-list"><li style="list-style-type:disc">Quantization (help from James h isere)</li></ul><ul id="2c2d68f4-b9f3-42c0-a810-5d95b10012ca" class="bulleted-list"><li style="list-style-type:disc">fast ray-tracing<ul id="b1e0c4a7-0556-4499-b5f6-e419f9696733" class="bulleted-list"><li style="list-style-type:circle">not going over this due to time constraints and the deep learning focus of this course</li></ul><ul id="7a46c3b1-b6c1-4c24-88f3-6c9974d4d123" class="bulleted-list"><li style="list-style-type:circle"><a href="https://en.wikipedia.org/wiki/Ray_tracing_(graphics)">https://en.wikipedia.org/wiki/Ray_tracing_(graphics)</a></li></ul></li></ul><ul id="8b7d77c2-0a1d-475a-9696-7495ec38fc02" class="bulleted-list"><li style="list-style-type:disc"><span style="border-bottom:0.05em solid">C++</span> vs <span style="border-bottom:0.05em solid">C</span> vs <span style="border-bottom:0.05em solid">CUDA Kernels</span> vs <span style="border-bottom:0.05em solid">CUDA API</span> vs <span style="border-bottom:0.05em solid">Python</span> vs <span style="border-bottom:0.05em solid">PyTorch</span> vs <span style="border-bottom:0.05em solid">Triton</span> </li></ul><ul id="66216bb3-bd12-493c-bd48-6afa444ab8d9" class="bulleted-list"><li style="list-style-type:disc">old profiling <code>nvprof</code> commands for older GPU architectures</li></ul><ul id="d2cc2660-b0f3-4a9e-87ed-17cbe6270291" class="bulleted-list"><li style="list-style-type:disc">hardware architecture</li></ul><ul id="24ad51ba-3b63-43cb-93b1-7a498464b76a" class="bulleted-list"><li style="list-style-type:disc">we will avoid the random libraries when experimenting to ensure reproducability (convert pytorch inits to CUDA inits)</li></ul><ul id="e4c66adc-3fbd-4ef8-88e4-41ff44821ee4" class="bulleted-list"><li style="list-style-type:disc">MLP backward</li></ul><ul id="61ca3b23-c592-4dc6-a85e-b752f5541586" class="bulleted-list"><li style="list-style-type:disc">avg / max pooling</li></ul><ul id="f867e8bf-b3fa-414a-b9d3-463723565731" class="bulleted-list"><li style="list-style-type:disc">we understand how simple MLPs work, now move down a level into tensor based autograd.</li></ul><ul id="9724cc1a-c4fb-441e-929e-29bcf681de06" class="bulleted-list"><li style="list-style-type:disc">what are these jacobian &amp; hessian matrices?</li></ul><p id="819024c9-a5b9-4d62-844d-c748e02e870d" class="">questioning whether cuDNN is worth learning because of the cuDNN pytorch bindings. if you need to write a custom kernel just do it manually for best performance. I thought it would still be smart to touch on the surface level functions of the cuXXXX libs because of general purpose GPU programming (tasks other than deep learning require the higher level API). </p><p id="6c2679e9-369f-411c-bff6-c76c004ae3e1" class="">cuDNN is highly optimized by nvidia hardware engineers so you want to take advantage of that when you can. sometimes your version of a cuda kernel isn’t the fastest. these APIs give you the current fastest implementation of an algo. if you are experimenting, this might not be the way to go (black box). </p><p id="6ff41098-2c4d-41e3-97e2-06099885c944" class="">Custom cuda kernels can be faster than pytorch if optimized carefully for a device architecture. the tradeoff has to be worth it. mostly used for research purposes since some functions don’t exist in the API. used in prod training + inference. can be generalized alike cuDNN. </p><figure id="37168469-dde3-4e8f-8111-dc329b39f4af" class="link-to-page"><a href="https://www.notion.so/kasun-s-course-37168469dde34e8f8111dc329b39f4af?pvs=21">kasun’s course</a></figure></div></details><details open=""><summary style="font-weight:600;font-size:1.5em;line-height:1.3;margin:0">Final Project</summary><div class="indented"><ul id="35a50f82-d2c8-4d90-b79d-e62a9f763fc2" class="toggle"><li><details open=""><summary>Project Intro/Desc — What are we doing exactly?</summary><ul id="dce0bb20-8c6a-418d-b392-aed756eb4dd6" class="bulleted-list"><li style="list-style-type:disc">MNIST CNN implementation in CUDA kernels (cuBLAS, cuDNN, cuFFT, manually optimized kernels, etc)</li></ul><ul id="0cafbb38-7363-4c8b-9dff-ac6737ed0c57" class="bulleted-list"><li style="list-style-type:disc">The end goal is to make our custom GPU code run faster than pytorch. we measure the time for a single batch load + forward pass + backward pass + weight update + zerograd across 100 or so iterations and take the average time. then compare it to our average time in pytorch and see who wins. optimize via profiling we learned in a past lecture</li></ul><ul id="2cb94a40-6215-445f-a207-dbde3a112220" class="toggle"><li><details open=""><summary>Components</summary><ul id="19cea37a-6e98-42d9-95c9-8aab64bf1585" class="toggle"><li><details open=""><summary>Dataloader</summary><ul id="0675da1b-81ba-4dcb-8dbb-fb0f0ca4f13a" class="bulleted-list"><li style="list-style-type:disc">look into the <a href="http://mnist.py">mnist.py</a> script under the hood taking care of mnist download + mgmt</li></ul><ul id="e303d4d3-353b-402a-9020-0d7bcac4e8f8" class="bulleted-list"><li style="list-style-type:disc">save everything to system DRAM to make dataloading as fast as possible</li></ul></details></li></ul><ul id="5b097b6c-0b37-41f7-b9c1-ec13ee889174" class="toggle"><li><details open=""><summary>Architecture</summary><ul id="fc11e7c9-d4d0-495f-bfe0-d276662f9067" class="bulleted-list"><li style="list-style-type:disc">CrossEntropyLoss<ul id="3f78e4e7-3938-423c-9591-5c3b34a71b7e" class="bulleted-list"><li style="list-style-type:circle">large batch size</li></ul></li></ul><ul id="7a9dd715-f06a-4d80-8282-d01bf80a40e9" class="bulleted-list"><li style="list-style-type:disc">SGD<ul id="a39d74af-0b54-47bd-b2c8-77136548550d" class="bulleted-list"><li style="list-style-type:circle">no momentum</li></ul><ul id="5d1a34be-eb01-4e0c-82f4-d370a88b9643" class="bulleted-list"><li style="list-style-type:circle">no LR schedulers</li></ul><ul id="8376fcce-3441-4084-bf2d-1a802f460886" class="bulleted-list"><li style="list-style-type:circle">no running variables</li></ul></li></ul><ul id="1e2275d5-d4eb-4c50-bb20-8c8bf3178143" class="bulleted-list"><li style="list-style-type:disc">CNN block<ul id="812fb8b9-96a3-425b-b171-9b52662fb60c" class="bulleted-list"><li style="list-style-type:circle">conv2d ⇒ batchnorm2d ⇒ relu ⇒ maxpool2d</li></ul></li></ul><ul id="62477004-fdcb-49b0-8d08-71852e474a43" class="bulleted-list"><li style="list-style-type:disc">MLP block<ul id="a05c0d5e-4561-49c5-ba26-bfb17bc846cd" class="bulleted-list"><li style="list-style-type:circle">linear ⇒ relu ⇒ linear</li></ul></li></ul><ul id="549a02f2-257e-4868-81ce-4f4725819433" class="bulleted-list"><li style="list-style-type:disc">Combined (omit residual connection because its a shallow network)<ul id="5f27ec83-ddc1-42fc-aa46-bb2a4bbbf2d7" class="bulleted-list"><li style="list-style-type:circle">CNN block 1</li></ul><ul id="714f6c79-e238-422d-b546-bd564b620959" class="bulleted-list"><li style="list-style-type:circle">CNN block 2</li></ul><ul id="f542c209-f495-4277-9f15-00f3dd869e77" class="bulleted-list"><li style="list-style-type:circle">flatten</li></ul><ul id="d696d164-4af4-458c-a11c-35979375219c" class="bulleted-list"><li style="list-style-type:circle">mlp block</li></ul><ul id="0dfe6687-40d8-40bd-af27-c723c3bf17fe" class="bulleted-list"><li style="list-style-type:circle">log softmax out (we are using cross entropy loss function so it makes sense to output logits)</li></ul></li></ul></details></li></ul><ul id="90e539b5-beed-4cf6-8d3d-e28ac1ab74ce" class="toggle"><li><details open=""><summary>PLACEHOLDER</summary></details></li></ul></details></li></ul></details></li></ul><ul id="5ee1df23-0e96-43d4-aa7b-3a3a59232537" class="toggle"><li><details open=""><summary>Python version</summary><ul id="23012b81-ffa8-4052-abe1-97a73c249201" class="bulleted-list"><li style="list-style-type:disc">pytorch profiler to compare performance on certain ops later (opt.). or could we just use the <code>time</code> library</li></ul></details></li></ul><ul id="126de040-0bd0-4a41-8b6b-cac8562c78b3" class="toggle"><li><details open=""><summary>CUDA C/C++</summary><ul id="289cf384-7c18-40c4-8f2d-575f079ee0ea" class="toggle"><li><details open=""><summary>transfer to CPU functions in our <code>.cu</code> script</summary><ul id="e614ad79-230c-4319-b09b-d111f5f9e40e" class="bulleted-list"><li style="list-style-type:disc">write the dataloader first (test with opencv and the print_tensor_image function)</li></ul></details></li></ul><ul id="72be8599-ea13-494d-a640-a52de5f8a32c" class="bulleted-list"><li style="list-style-type:disc">naive kernel from the CPU function design</li></ul><ul id="e74655c0-dbcd-41e6-b656-0a3268b6f1a9" class="bulleted-list"><li style="list-style-type:disc">faster and faster kernels progressively</li></ul></details></li></ul><ul id="ede3c9d7-6909-4dc1-bf6e-6667e49e6b8f" class="toggle"><li><details open=""><summary>Link to mnist-cuda</summary></details></li></ul><p id="5a22e272-d6a8-43c4-94f9-356d6e599e85" class="">
</p></div></details></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>